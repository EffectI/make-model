{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf72c14f",
   "metadata": {},
   "source": [
    "Train 데이터 셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# DATA_DIR = '/data'\n",
    "# ORIGIN_TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
    "# SEED = 42\n",
    "\n",
    "\n",
    "# df = pd.read_csv(ORIGIN_TRAIN_PATH)\n",
    "# print(f\"원본 데이터 개수: {len(df)}개\")\n",
    "\n",
    "# train_val, local_test = train_test_split(\n",
    "#     df, test_size=0.1, stratify=df['label'], random_state=SEED\n",
    "# )\n",
    "\n",
    "# train, valid = train_test_split(\n",
    "#     train_val, test_size=0.11, stratify=train_val['label'], random_state=SEED\n",
    "# )\n",
    "\n",
    "# print(f\"분할 완료: Train({len(train)}) / Valid({len(valid)}) / Local_Test({len(local_test)})\")\n",
    "\n",
    "# train.to_csv(os.path.join(DATA_DIR, 'train_fixed.csv'), index=False)\n",
    "# valid.to_csv(os.path.join(DATA_DIR, 'valid_fixed.csv'), index=False)\n",
    "# local_test.to_csv(os.path.join(DATA_DIR, 'local_test_fixed.csv'), index=False)\n",
    "\n",
    "# print(\"데이터 분할 및 저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207db44e",
   "metadata": {},
   "source": [
    "특수 기호 제거 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc5e2cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "📂 읽어올 경로: c:\\Users\\Pyo\\OneDrive\\바탕 화면\\git\\make-model\\data\\raw\\train.csv\n",
      "✅ 데이터 로드 성공 (utf-8): train.csv\n",
      "텍스트 컬럼 감지됨: 'full_text'\n",
      "데이터 정제 시작...\n",
      "\n",
      "==============================\n",
      "      정제 결과 리포트      \n",
      "==============================\n",
      "총 데이터 개수 : 97172개\n",
      "변경된 데이터  : 97041개 (99.87%)\n",
      "\n",
      "[변경 예시 Top 3]\n",
      "전: 카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5km2, 높이는 452m이다. 하와이 제도에서 가장 작은 화산섬이자 무인도이며 길이는 18km, 너비는 10km이다. \n",
      " 마우이섬에서 남서쪽으로 약 11km 정도 떨어진 곳에 위치하며 라나이섬의 남동쪽에 위치한다. 고도가 낮고 북동쪽에서 불어오는 무역풍을 통해 산악 지대에서 내리는 비를 형성하지 못하기 때문에 건조한 기후를 띤다. 마우이섬 화산의 비그늘에 속해 있기 때문에 섬 전체 면적의 1/4 이상이 부식되어 있다. \n",
      " 1000년경부터 사람이 거주했으며 해안 지대에는 소규모 임시 어촌이 형성되었다. 섬 안에는 주민들이 돌로 만든 제단에서 종교 의식을 거행한 흔적들, 주민들이 암석이나 평평한 돌에 그림을 그린 흔적들이 남아 있다. 1778년부터 1800년대까지 이 지역을 지나 항해하던 사람들의 보고에 따르면 카호올라웨섬은 무인도였고 나무도 물도 없는 불모지였다고 한다. \n",
      " 1830년대에는 하와이 왕국의 카메하메하 3세 국왕에 의해 남자 죄수들의 유형지로 사용되었지만 1853년에 폐지되었다. 1858년에는 하와이 정부가 목장 사업가들에게 카호올라웨섬을 양도했지만 가뭄과 과도한 방목으로 인해 땅이 말라갔다. 또한 강한 무역풍으로 인해 표토의 대부분이 날아가면서 붉은 경반층만 남게 되었다. \n",
      " 1910년부터 1918년까지 하와이 준주가 섬의 원래 모습을 복원하기 위해 이 섬을 천연보호구역으로 지정했지만 큰 성과를 거두지 못했다. \n",
      " 1941년 12월 7일에 일어난 일본 제국 해군의 진주만 공격을 계기로 카호올라웨섬은 태평양 전쟁에 참전한 미국 병사들의 훈련소로 사용되었다. 1981년 3월 18일에는 미국 국립사적지에 등재되었다.\n",
      "후: 카호올라웨섬은 하와이 제도를 구성하는 8개의 화산섬 가운데 하나로 면적은 115.5km2, 높이는 452m이다. 하와이 제도에서 가장 작은 화산섬이자 무인도이며 길이는 18km, 너비는 10km이다. 마우이섬에서 남서쪽으로 약 11km 정도 떨어진 곳에 위치하며 라나이섬의 남동쪽에 위치한다. 고도가 낮고 북동쪽에서 불어오는 무역풍을 통해 산악 지대에서 내리는 비를 형성하지 못하기 때문에 건조한 기후를 띤다. 마우이섬 화산의 비그늘에 속해 있기 때문에 섬 전체 면적의 1/4 이상이 부식되어 있다. 1000년경부터 사람이 거주했으며 해안 지대에는 소규모 임시 어촌이 형성되었다. 섬 안에는 주민들이 돌로 만든 제단에서 종교 의식을 거행한 흔적들, 주민들이 암석이나 평평한 돌에 그림을 그린 흔적들이 남아 있다. 1778년부터 1800년대까지 이 지역을 지나 항해하던 사람들의 보고에 따르면 카호올라웨섬은 무인도였고 나무도 물도 없는 불모지였다고 한다. 1830년대에는 하와이 왕국의 카메하메하 3세 국왕에 의해 남자 죄수들의 유형지로 사용되었지만 1853년에 폐지되었다. 1858년에는 하와이 정부가 목장 사업가들에게 카호올라웨섬을 양도했지만 가뭄과 과도한 방목으로 인해 땅이 말라갔다. 또한 강한 무역풍으로 인해 표토의 대부분이 날아가면서 붉은 경반층만 남게 되었다. 1910년부터 1918년까지 하와이 준주가 섬의 원래 모습을 복원하기 위해 이 섬을 천연보호구역으로 지정했지만 큰 성과를 거두지 못했다. 1941년 12월 7일에 일어난 일본 제국 해군의 진주만 공격을 계기로 카호올라웨섬은 태평양 전쟁에 참전한 미국 병사들의 훈련소로 사용되었다. 1981년 3월 18일에는 미국 국립사적지에 등재되었다.\n",
      "--------------------\n",
      "전: 천문학에서 청색거성(靑色巨星, )은 광도 분류에서 III형(거성) 또는 II형(밝은 거성)인 뜨거운 별이다. 이러한 별들은 표준 헤르츠스프룽-러셀 도표에서 주계열의 약간 오른쪽 위에 놓여있다. \n",
      " 용어는 각자 다른 진화 단계에 있는 여러 가지 별에 적용되는데, 이들 모두 주계열에서 진화한 별이지만 약간씩 다른 공통점을 가진다. 그래서 청색거성은 특정한 유형의 별보다는 단순하게 HR 도표의 특정 영역에 있는 별을 표현한다. 청색거성은 무겁고 덜 흔한 별에서 진화하며, 이 단계에서의 수명이 짧기 때문에 적색거성보다 훨씬 희귀하다. \n",
      " 청색거성이라는 명칭은 종종 매우 크고 뜨거운 주계열성과 같이, 다른 무겁고 밝은 별을 일컫는데 잘못 사용되기도 한다. \n",
      " 청색거성은 엄격히 정의된 단어가 아니어서 서로 다른 다양한 유형의 별에 폭넓게 사용되었다. 이들이 가진 공통점은 동일한 질량 및 온도의 주계열성과 비교해서 크기와 광도가 조금 증가했다는 점과, 청색으로 불리기에 충분하는 점이다. 이는 분광형이 O형, B형, 때때로 A형 초반임을 의미한다. 이들의 표면온도는 약 10,000 K 이상이고, 영년 주계열성일 때의 질량은 태양(M)보다 두 배 이상 크며, 절대등급은 0등급 이상이다. 이러한 별들의 반지름은 태양의 반지름(R)의 100배까지인 적색거성에 비해서, 고작 5~10배이다. \n",
      " 청색거성으로 표현되는 매우 차갑고 어두운 별은, 적색거성 단계를 지난 중간 질량의 별이 현재 핵에서 헬륨을 연소하고 있는 단계인 수평가지에 있다. 질량과 화학 조성에 기반하여 이러한 별들은 서서히 청색쪽으로 이동한다. 중심핵에의 헬륨이 고갈되면 다시 적색쪽으로 이동하여 점근거성가지(AGB)에 위치하게 된다. 보통 분광형 A형의 거문고자리 RR형 변광성은 수평가지의 중간 쪽에 위치하는데, 거문고자리 RR형 변광성보다 뜨거운 수평가지의 별은 보통 청색거성으로 간주된다. 때때로 거문고자리 RR형 변광성 자체도 일부가 F형이라도 청색거성으로 불리기도 한다. 매우 뜨거운 청색수평가지(BHB)의 별들은 극수평가지(EHB) 별로 불리는데, 동일한 광도의 주계열성보다 뜨겁다. 그런 경우에 청색거성보다는 청색준왜성(sdB)이라고 불린다. 이는 주계열성일 때와 비교해서 증가한 광도와 온도 대신에 HR 도표에서 주계열의 왼쪽에 위치해 있어서 붙인 이름이다. \n",
      " 거성에 대해서 명확한 최대 한계는 없지만, O형 초반의 주계열성과 그와 거의 동일한 크기와 온도의 초거성을 별도로 분류하는데 점점 어려워지고 있다. 적당한 예로 50 M, 30,000 K를 넘으며 태양의 광도(L)의 10만 배가 넘는 O형 거성 둘로 구성된 근접쌍성인 플라스켓의 별이 있다. 천문학자들은 적어도 별 중 하나를 선 스펙트럼에서의 미묘한 차이에 근거해서 초거성으로 분류해야 할지 말지 아직도 의견이 분분하다. \n",
      " HR 도표의 청색거성 영역에서 발견되는 별은 일생에서의 단계가 각자 크게 다를 수 있으나, 모두 중심핵의 수소 연료를 거의 고갈한 후주계열성이다. \n",
      " 가장 간단한 경우에서 뜨겁고 밝은 별은 중심핵의 수소가 고갈될 때 팽창하기 시작하여 점점 밝고 차가워지면서 처음에는 청색준거성이 되고 나중에 청색거성이 된다. 중간 정도의 질량을 가진 별들은 적색거성이 될 때까지 팽창과 냉각을 지속할 것이다. 무거운 별 역시 수소껍질연소 과정으로 계속 팽창하지만, 거의 일정한 광도로 HR 도표에서 수평 방향으로 이동한다. 이리해서 이들은 적색초거성이 될 때까지 청색거성, 청색휘거성, 청색초거성, 황색초거성 단계를 빠르게 통과한다. 이러한 별들의 광도 분류는 별의 표면중력에 민감한 선 스펙트럼으로 결정되는데, 크게 팽창한 밝은 별은 I형(초거성), 어느정도 덜 팽창한 밝은 별은 II형 또는 III형으로 분류된다. 많은 청색거성은 수명이 짧은 무거운 별이기 때문에 어린 별들로 느슨하게 속박된 큰 별무리인 O-B 성협에서 발견될 수 있다. \n",
      " BHB 별은 아직 커다란 수소껍질을 가지고 있다 해도 더 진화하여 헬륨 연소핵을 가진다. 또한 이들은 약 5~10M의 적당한 질량을 가지고 있기 때문에 더욱 무거운 청색거성보다 훨씬 오래된 경우가 많다. BHB는 주로 오래된 성단의 색-등급도에서, 헬륨 연소핵을 가지는 동일한 연령의 별들이 거의 같은 밝기에 온도가 다르게 발견되는 수평 영역에서 이름 붙여졌다. 또한 이러한 별들은 일정한 광도에 온도가 증가하면서 헬륨 연소핵 단계를 통과하여 진화하는데, 나중에 다시 AGB로 이동함으로써 온도는 감소한다. 그러나 수평가지의 청색 끝부분에서는 어두운 별의 \"청색 꼬리\"(\"blue tail\")와, 가끔 훨씬 더 뜨거운 별들의 \"청색 갈고리\"(\"blue hook\")가 만들어진다. \n",
      " 보통 청색거성으로 불리지 않는 더 진화한 뜨거운 별들도 있는데, 매우 밝고 극단적인 온도에 주로 헬륨과 질소 방출선으로 구별되는 울프-레이에 별과, 행성상성운을 형성하며 울프-레이에 별과 유사하지만 그보다 작고 가벼운 후-AGB 별이 있다. 그밖에도 성단의 주계열에서 뚜렷하게 관측되는 밝은 청색 별로 이러한 광도의 주계열성은 성단의 나이를 따지면 이미 거성이나 초거성으로 진화했어야 하는 청색낙오성과, 청색거성을 지나서 진화한 매우 무거운 별으로 스펙트럼의 큰 팽창 효과로 구별되는 청색초거성이 있다. \n",
      " 순전히 이론적인 영역의 별은 적색왜성이 최종적으로 수조 년이 지난 미래에 중심핵의 수소를 소진할 때 형성될 것이다. 적색왜성은 중심부까지 대류하여 헬륨을 더욱 더 응집함으로써 종래에 더 이상 핵융합을 유지할 수 없어 백색왜성으로 빠르게 붕괴할 때까지 매우 느리게 온도와 광도가 증가하는 것으로 추측된다. 이러한 별들이 태양보다 뜨거워질 수 있기는 해도 그보다 밝아질 수는 없다. 그래서 이들은 우리가 보는 오늘날에 보는 청색거성이라고 할 수 없다. 그러한 별들은 쉽게 혼동될 수 있어도 청색왜성이라고 이름 붙여졌다.\n",
      "후: 천문학에서 청색거성은 광도 분류에서 III형(거성) 또는 II형(밝은 거성)인 뜨거운 별이다. 이러한 별들은 표준 헤르츠스프룽-러셀 도표에서 주계열의 약간 오른쪽 위에 놓여있다. 용어는 각자 다른 진화 단계에 있는 여러 가지 별에 적용되는데, 이들 모두 주계열에서 진화한 별이지만 약간씩 다른 공통점을 가진다. 그래서 청색거성은 특정한 유형의 별보다는 단순하게 HR 도표의 특정 영역에 있는 별을 표현한다. 청색거성은 무겁고 덜 흔한 별에서 진화하며, 이 단계에서의 수명이 짧기 때문에 적색거성보다 훨씬 희귀하다. 청색거성이라는 명칭은 종종 매우 크고 뜨거운 주계열성과 같이, 다른 무겁고 밝은 별을 일컫는데 잘못 사용되기도 한다. 청색거성은 엄격히 정의된 단어가 아니어서 서로 다른 다양한 유형의 별에 폭넓게 사용되었다. 이들이 가진 공통점은 동일한 질량 및 온도의 주계열성과 비교해서 크기와 광도가 조금 증가했다는 점과, 청색으로 불리기에 충분하는 점이다. 이는 분광형이 O형, B형, 때때로 A형 초반임을 의미한다. 이들의 표면온도는 약 10,000 K 이상이고, 영년 주계열성일 때의 질량은 태양(M)보다 두 배 이상 크며, 절대등급은 0등급 이상이다. 이러한 별들의 반지름은 태양의 반지름(R)의 100배까지인 적색거성에 비해서, 고작 5~10배이다. 청색거성으로 표현되는 매우 차갑고 어두운 별은, 적색거성 단계를 지난 중간 질량의 별이 현재 핵에서 헬륨을 연소하고 있는 단계인 수평가지에 있다. 질량과 화학 조성에 기반하여 이러한 별들은 서서히 청색쪽으로 이동한다. 중심핵에의 헬륨이 고갈되면 다시 적색쪽으로 이동하여 점근거성가지(AGB)에 위치하게 된다. 보통 분광형 A형의 거문고자리 RR형 변광성은 수평가지의 중간 쪽에 위치하는데, 거문고자리 RR형 변광성보다 뜨거운 수평가지의 별은 보통 청색거성으로 간주된다. 때때로 거문고자리 RR형 변광성 자체도 일부가 F형이라도 청색거성으로 불리기도 한다. 매우 뜨거운 청색수평가지(BHB)의 별들은 극수평가지(EHB) 별로 불리는데, 동일한 광도의 주계열성보다 뜨겁다. 그런 경우에 청색거성보다는 청색준왜성(sdB)이라고 불린다. 이는 주계열성일 때와 비교해서 증가한 광도와 온도 대신에 HR 도표에서 주계열의 왼쪽에 위치해 있어서 붙인 이름이다. 거성에 대해서 명확한 최대 한계는 없지만, O형 초반의 주계열성과 그와 거의 동일한 크기와 온도의 초거성을 별도로 분류하는데 점점 어려워지고 있다. 적당한 예로 50 M, 30,000 K를 넘으며 태양의 광도(L)의 10만 배가 넘는 O형 거성 둘로 구성된 근접쌍성인 플라스켓의 별이 있다. 천문학자들은 적어도 별 중 하나를 선 스펙트럼에서의 미묘한 차이에 근거해서 초거성으로 분류해야 할지 말지 아직도 의견이 분분하다. HR 도표의 청색거성 영역에서 발견되는 별은 일생에서의 단계가 각자 크게 다를 수 있으나, 모두 중심핵의 수소 연료를 거의 고갈한 후주계열성이다. 가장 간단한 경우에서 뜨겁고 밝은 별은 중심핵의 수소가 고갈될 때 팽창하기 시작하여 점점 밝고 차가워지면서 처음에는 청색준거성이 되고 나중에 청색거성이 된다. 중간 정도의 질량을 가진 별들은 적색거성이 될 때까지 팽창과 냉각을 지속할 것이다. 무거운 별 역시 수소껍질연소 과정으로 계속 팽창하지만, 거의 일정한 광도로 HR 도표에서 수평 방향으로 이동한다. 이리해서 이들은 적색초거성이 될 때까지 청색거성, 청색휘거성, 청색초거성, 황색초거성 단계를 빠르게 통과한다. 이러한 별들의 광도 분류는 별의 표면중력에 민감한 선 스펙트럼으로 결정되는데, 크게 팽창한 밝은 별은 I형(초거성), 어느정도 덜 팽창한 밝은 별은 II형 또는 III형으로 분류된다. 많은 청색거성은 수명이 짧은 무거운 별이기 때문에 어린 별들로 느슨하게 속박된 큰 별무리인 O-B 성협에서 발견될 수 있다. BHB 별은 아직 커다란 수소껍질을 가지고 있다 해도 더 진화하여 헬륨 연소핵을 가진다. 또한 이들은 약 5~10M의 적당한 질량을 가지고 있기 때문에 더욱 무거운 청색거성보다 훨씬 오래된 경우가 많다. BHB는 주로 오래된 성단의 색-등급도에서, 헬륨 연소핵을 가지는 동일한 연령의 별들이 거의 같은 밝기에 온도가 다르게 발견되는 수평 영역에서 이름 붙여졌다. 또한 이러한 별들은 일정한 광도에 온도가 증가하면서 헬륨 연소핵 단계를 통과하여 진화하는데, 나중에 다시 AGB로 이동함으로써 온도는 감소한다. 그러나 수평가지의 청색 끝부분에서는 어두운 별의 \"청색 꼬리\"(\"blue tail\")와, 가끔 훨씬 더 뜨거운 별들의 \"청색 갈고리\"(\"blue hook\")가 만들어진다. 보통 청색거성으로 불리지 않는 더 진화한 뜨거운 별들도 있는데, 매우 밝고 극단적인 온도에 주로 헬륨과 질소 방출선으로 구별되는 울프-레이에 별과, 행성상성운을 형성하며 울프-레이에 별과 유사하지만 그보다 작고 가벼운 후-AGB 별이 있다. 그밖에도 성단의 주계열에서 뚜렷하게 관측되는 밝은 청색 별로 이러한 광도의 주계열성은 성단의 나이를 따지면 이미 거성이나 초거성으로 진화했어야 하는 청색낙오성과, 청색거성을 지나서 진화한 매우 무거운 별으로 스펙트럼의 큰 팽창 효과로 구별되는 청색초거성이 있다. 순전히 이론적인 영역의 별은 적색왜성이 최종적으로 수조 년이 지난 미래에 중심핵의 수소를 소진할 때 형성될 것이다. 적색왜성은 중심부까지 대류하여 헬륨을 더욱 더 응집함으로써 종래에 더 이상 핵융합을 유지할 수 없어 백색왜성으로 빠르게 붕괴할 때까지 매우 느리게 온도와 광도가 증가하는 것으로 추측된다. 이러한 별들이 태양보다 뜨거워질 수 있기는 해도 그보다 밝아질 수는 없다. 그래서 이들은 우리가 보는 오늘날에 보는 청색거성이라고 할 수 없다. 그러한 별들은 쉽게 혼동될 수 있어도 청색왜성이라고 이름 붙여졌다.\n",
      "--------------------\n",
      "전: 엘자스-로트링겐 평의회 공화국은 1차대전 말기 독일 혁명 와중에 엘자스-로트링겐에서 수립된 단명한 평의회 공화국이다. \n",
      " 엘자스-로트링겐 출신 병사들은 1918년 초부터 이미 불온한 낌새를 보이기 시작하여, 1918년 5월 12일 베베를로 기지에서 알자스인 병사들의 항명반란이 일어나기도 했다. \n",
      " 1918년 10월, 독일 황립해군의 수병들이 영국 왕립해군과 싸우기 위해 출항하라는 명령에 항명하면서 킬 군항의 반란이 터졌고, 군항을 접수한 수병들이 노동조합원들과 합류한 뒤 독일 전역으로 혁명이 확산됨으로써 불과 며칠 사이 군주정이 전복되었다. 당시 엘자스인 및 로트링겐인 병사 15,000 명이 해군에서 복무하고 있었고, 그들 중 상당수가 군항반란에 참여한 뒤 고향에서도 혁명을 일으키고자 했다. \n",
      " 11월 8일, 바이에른에서 평의회 공화국이 선포되었다는 소식이 엘자스의 수도 슈트라스부르크에 전해졌고, 여기에 영감을 받은 수천 명의 시위자들이 슈트라스부르크의 중앙광장인 클레베르 광장을 행진했다. 북독일에서 열차를 타고 귀환하던 엘자스-로트링겐 출신 수병들은 켈 교량에서 왕당파 장교들에게 가로막혔고, 왕당파 측이 사격을 가하여 수병들 중 한 명이 사망했으나 결국 혁명군이 켈을 접수했다. 이후 슈트라스부르크에서 엘자스-로트링겐 전역으로 혁명이 확산되었고, 슈트라스부르크, 하게나우, 뮐하우젠, 슐레트스타트, 콜마르, 메츠 등 도시들에 평의회가 세워졌다. \n",
      " 수병들은 슈트라스부르크 병사평의회를 설치하고 도시 행정을 접수했다. 이후 양조장 노동조합이 참여하여 노동자-병사 평의회가 수립되었고, 대성당 첨탑을 비롯한 도시 전역에 적기가 휘날렸다. 사면령이 선포되고, 언론의 자유가 천명되었다. 공장 노동자들은 임금인상을 요구하며 파업했고, 평의회는 공장주들의 반대를 묵살하고 임금을 인상했다. 독일사회민주당 슈트라스부르크시당 지도자였던 자크 페로테는 프랑스군 장군들에게 군대를 보내 질서를 바로잡아 달라고 요청했다. \n",
      " 11일 뒤, 프랑스군이 엘자스-로트링겐을 점령하고 합병했다. 앙리 조제프 외젠 구로 장군의 제4군 병사들이 11월 22일 슈트라스부르크 교외에 도달했고, 파업을 무력으로 분쇄한 뒤 좌익 선동자들을 체포했다. 프랑스 제3공화국은 엘자스-로트링겐 인민들이 스스로 쟁취한 자치를 박탈하고 프랑스적 중앙집권으로 동화시켰다.\n",
      "후: 엘자스-로트링겐 평의회 공화국은 1차대전 말기 독일 혁명 와중에 엘자스-로트링겐에서 수립된 단명한 평의회 공화국이다. 엘자스-로트링겐 출신 병사들은 1918년 초부터 이미 불온한 낌새를 보이기 시작하여, 1918년 5월 12일 베베를로 기지에서 알자스인 병사들의 항명반란이 일어나기도 했다. 1918년 10월, 독일 황립해군의 수병들이 영국 왕립해군과 싸우기 위해 출항하라는 명령에 항명하면서 킬 군항의 반란이 터졌고, 군항을 접수한 수병들이 노동조합원들과 합류한 뒤 독일 전역으로 혁명이 확산됨으로써 불과 며칠 사이 군주정이 전복되었다. 당시 엘자스인 및 로트링겐인 병사 15,000 명이 해군에서 복무하고 있었고, 그들 중 상당수가 군항반란에 참여한 뒤 고향에서도 혁명을 일으키고자 했다. 11월 8일, 바이에른에서 평의회 공화국이 선포되었다는 소식이 엘자스의 수도 슈트라스부르크에 전해졌고, 여기에 영감을 받은 수천 명의 시위자들이 슈트라스부르크의 중앙광장인 클레베르 광장을 행진했다. 북독일에서 열차를 타고 귀환하던 엘자스-로트링겐 출신 수병들은 켈 교량에서 왕당파 장교들에게 가로막혔고, 왕당파 측이 사격을 가하여 수병들 중 한 명이 사망했으나 결국 혁명군이 켈을 접수했다. 이후 슈트라스부르크에서 엘자스-로트링겐 전역으로 혁명이 확산되었고, 슈트라스부르크, 하게나우, 뮐하우젠, 슐레트스타트, 콜마르, 메츠 등 도시들에 평의회가 세워졌다. 수병들은 슈트라스부르크 병사평의회를 설치하고 도시 행정을 접수했다. 이후 양조장 노동조합이 참여하여 노동자-병사 평의회가 수립되었고, 대성당 첨탑을 비롯한 도시 전역에 적기가 휘날렸다. 사면령이 선포되고, 언론의 자유가 천명되었다. 공장 노동자들은 임금인상을 요구하며 파업했고, 평의회는 공장주들의 반대를 묵살하고 임금을 인상했다. 독일사회민주당 슈트라스부르크시당 지도자였던 자크 페로테는 프랑스군 장군들에게 군대를 보내 질서를 바로잡아 달라고 요청했다. 11일 뒤, 프랑스군이 엘자스-로트링겐을 점령하고 합병했다. 앙리 조제프 외젠 구로 장군의 제4군 병사들이 11월 22일 슈트라스부르크 교외에 도달했고, 파업을 무력으로 분쇄한 뒤 좌익 선동자들을 체포했다. 프랑스 제3공화국은 엘자스-로트링겐 인민들이 스스로 쟁취한 자치를 박탈하고 프랑스적 중앙집권으로 동화시켰다.\n",
      "--------------------\n",
      "\n",
      "==============================\n",
      "저장 완료!\n",
      "저장 경로: c:\\Users\\Pyo\\OneDrive\\바탕 화면\\git\\make-model\\data\\interim\\clean_train.csv\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "#  1. 경로 및 설정 (pathlib 적용)\n",
    "# ==========================================\n",
    "# 현재 작업 경로(Project Root)를 기준으로 경로를 잡음. 정상적인 경우 make-model이 베이스폴더가 됨\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# 입력 데이터 경로 (원본)\n",
    "INPUT_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "INPUT_FILE_NAME = 'train.csv'\n",
    "\n",
    "# 출력 데이터 경로 (저장용) - interim 폴더로 분리\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'data' / 'interim'\n",
    "OUTPUT_FILE_NAME = 'clean_train.csv'\n",
    "\n",
    "# ==========================================\n",
    "#  2. Cleaning 함수 정의\n",
    "# ==========================================\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. HTML 태그 제거\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # 2. 한자 제거\n",
    "    text = re.sub(r'[\\u4E00-\\u9FFF]', '', text)\n",
    "\n",
    "    # 3. 빈 괄호 및 내용 없는 괄호 패턴 제거\n",
    "    text = re.sub(r'\\(\\s*[^\\w가-힣]*\\s*\\)', '', text)\n",
    "\n",
    "    # 4. ( ? ~ ? ) 형태의 특정 노이즈 패턴 제거\n",
    "    text = re.sub(r'\\([^\\(\\)]{0,20}[\\?\\~]{1,3}[^\\(\\)]{0,20}\\)', '', text)\n",
    "\n",
    "    # 5. 반복되는 점(...) 정제 -> 점(.) 하나로 변경\n",
    "    text = re.sub(r'[.,]{3,}', '.', text)\n",
    "\n",
    "    # 6. 반복되는 쉼표(,,,) 정제 -> 쉼표(,) 하나로 변경\n",
    "    text = re.sub(r',\\s*,+', ',', text)\n",
    "\n",
    "    # 7. 중복 괄호((())) 정제 -> 괄호 하나로 변경\n",
    "    text = re.sub(r'\\({2,}', '(', text)\n",
    "    text = re.sub(r'\\){2,}', ')', text)\n",
    "\n",
    "    # 8. 다중 공백 제거 (하나의 공백으로)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# ==========================================\n",
    "#  3. 데이터 로드 함수\n",
    "# ==========================================\n",
    "def load_data(path):\n",
    "    if not path.exists():\n",
    "        print(f\"파일이 존재하지 않음: {path}\")\n",
    "        return None\n",
    "    \n",
    "    encodings = ['utf-8', 'utf-8-sig', 'cp949']\n",
    "    df = None\n",
    "    \n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            print(f\"데이터 로드 성공 ({enc}): {path.name}\")\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    if df is None:\n",
    "        print(\"데이터 로드 실패 (인코딩 문제일 수 있음)\")\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "#  4. 메인 실행 함수\n",
    "# ==========================================\n",
    "def main():\n",
    "    input_path = INPUT_DIR / INPUT_FILE_NAME\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"읽어올 경로: {input_path}\")\n",
    "    \n",
    "    df = load_data(input_path)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    text_col = 'full_text' \n",
    "    if 'full_text' not in df.columns:\n",
    "        for candidate in ['text', 'paragraph_text', 'content']:\n",
    "            if candidate in df.columns:\n",
    "                text_col = candidate\n",
    "                break\n",
    "    \n",
    "    print(f\"텍스트 컬럼 감지됨: '{text_col}'\")\n",
    "    print(\"데이터 정제 시작...\")\n",
    "\n",
    "    # 3. [리포트용] 변경 전 데이터 백업\n",
    "    original_col = f\"{text_col}_original\"\n",
    "    df[original_col] = df[text_col]\n",
    "\n",
    "    # 4. 정제 수행\n",
    "    df[text_col] = df[text_col].apply(clean_text)\n",
    "\n",
    "    # 5. [리포트용] 실제로 내용이 바뀐 행만 추출\n",
    "    changed_df = df[df[original_col] != df[text_col]]\n",
    "    \n",
    "    # --- 결과 리포트 출력 ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"      정제 결과 리포트      \")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"총 데이터 개수 : {len(df)}개\")\n",
    "    print(f\"변경된 데이터  : {len(changed_df)}개 ({(len(changed_df)/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    if len(changed_df) > 0:\n",
    "        print(\"\\n[변경 예시 Top 3]\")\n",
    "        for i, row in changed_df.head(3).iterrows():\n",
    "            print(f\"전: {row[original_col]}\")\n",
    "            print(f\"후: {row[text_col]}\")\n",
    "            print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"\\n(이미 깨끗해서 변경된 내용이 없습니다)\")\n",
    "\n",
    "    # 6. 빈 문자열 제거\n",
    "    initial_len = len(df)\n",
    "    df = df[df[text_col].str.strip() != \"\"]\n",
    "    removed_count = initial_len - len(df)\n",
    "    \n",
    "    if removed_count > 0:\n",
    "        print(f\"\\n정제 후 빈 문자열이 되어 제거된 행: {removed_count}개\")\n",
    "\n",
    "    # 7. 임시 컬럼 삭제\n",
    "    df.drop(columns=[original_col], inplace=True)\n",
    "\n",
    "    # 8. 저장 경로 설정 (폴더 없으면 자동 생성)\n",
    "    output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "    \n",
    "    # 부모 폴더(processed)가 없으면 생성\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 저장\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"저장 완료!\")\n",
    "    print(f\"저장 경로: {output_path}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ffae0",
   "metadata": {},
   "source": [
    "4fold용 데이터셋 분할\n",
    "1. train -> 0.9 : 0.1(train / test)\n",
    "2. 4fold 0.75 : 0.25(train / valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ab64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      ">>> 데이터 분할 작업 시작\n",
      "입력 파일: c:\\Users\\Pyo\\OneDrive\\바탕 화면\\git\\make-model\\data\\interim\\clean_train.csv\n",
      "출력 폴더: c:\\Users\\Pyo\\OneDrive\\바탕 화면\\git\\make-model\\data\\fold\n",
      "설정: Fold=4, Test Size=0.1\n",
      "----------------------------------------\n",
      "데이터 로드 완료: 97172개 행\n",
      "\n",
      "[Step 1] Test Set 분리 (비율: 0.1)\n",
      " -> Test Set 저장 완료: local_origin_test.csv\n",
      "    (Size: 9718, Label 0: 8918, Label 1: 800)\n",
      "\n",
      "[Step 2] 4-Fold Cross Validation 데이터 생성\n",
      " -> [fold0] 저장 완료\n",
      " -> [fold1] 저장 완료\n",
      " -> [fold2] 저장 완료\n",
      " -> [fold3] 저장 완료\n",
      "\n",
      ">>> 모든 작업이 완료됨.\n",
      "최종 결과물 위치: c:\\Users\\Pyo\\OneDrive\\바탕 화면\\git\\make-model\\data\\fold\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# ==========================================\n",
    "#  1. 경로 및 설정 (pathlib 적용)\n",
    "# ==========================================\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "INPUT_DIR = PROJECT_ROOT / 'data' / 'interim'\n",
    "INPUT_FILE_NAME = 'clean_train.csv'\n",
    "\n",
    "# 출력 경로: Fold 데이터가 저장될 위치 ('data/fold')\n",
    "OUTPUT_ROOT_DIR = PROJECT_ROOT / 'data' / 'fold'\n",
    "\n",
    "# 저장될 파일 이름 설정\n",
    "OUTPUT_TEST_FILENAME = 'local_origin_test.csv' \n",
    "OUTPUT_TRAIN_FILENAME = 'origin_train.csv'       \n",
    "OUTPUT_VALID_FILENAME = 'origin_valid.csv'       \n",
    "\n",
    "SEED = 42\n",
    "detected_delimiter = ','\n",
    "detected_quotechar = '\"'\n",
    "\n",
    "# ==========================================\n",
    "#  2. 유틸리티 함수\n",
    "# ==========================================\n",
    "\n",
    "def find_column_name(columns, candidates):\n",
    "    for col in columns:\n",
    "        if col.lower().strip() in candidates:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_fix_data(path, is_test=False):\n",
    "    # pathlib 객체 호환 처리\n",
    "    if not path.exists():\n",
    "        print(f\"파일이 없음: {path}\")\n",
    "        return None\n",
    "\n",
    "    df = None\n",
    "    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp949']\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                encoding=encoding,\n",
    "                engine='python',\n",
    "                on_bad_lines='skip',\n",
    "                encoding_errors='ignore',\n",
    "                delimiter=detected_delimiter,\n",
    "                quotechar=detected_quotechar,\n",
    "                quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        print(f\"데이터 로드 실패: {path}\")\n",
    "        return None\n",
    "\n",
    "    # 컬럼명 정리 (text)\n",
    "    text_candidates = ['paragraph_text', 'text', 'sentence', 'content', 'full_text']\n",
    "    text_col = find_column_name(df.columns, text_candidates)\n",
    "    if text_col:\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            df.rename(columns={obj_cols[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # 컬럼명 정리 (id) - Test셋일 경우\n",
    "    if is_test:\n",
    "        id_candidates = ['id', 'idx', 'index', 'no', 'ID']\n",
    "        id_col = find_column_name(df.columns, id_candidates)\n",
    "        if id_col:\n",
    "            df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "        else:\n",
    "            df['id'] = df.index\n",
    "\n",
    "    # 컬럼명 정리 (label) - Train셋일 경우\n",
    "    if not is_test:\n",
    "        target_candidates = ['generated', 'label', 'target', 'class']\n",
    "        target_col = find_column_name(df.columns, target_candidates)\n",
    "        if target_col:\n",
    "            df.rename(columns={target_col: 'label'}, inplace=True)\n",
    "            try:\n",
    "                df['label'] = df['label'].astype(int)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"타겟(Label) 컬럼을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_split_and_kfold(input_path, output_root, n_splits=4, test_size=0.1):\n",
    "    print(f\"-\" * 40)\n",
    "    print(f\">>> 데이터 분할 작업 시작\")\n",
    "    print(f\"입력 파일: {input_path}\")\n",
    "    print(f\"출력 폴더: {output_root}\")\n",
    "    print(f\"설정: Fold={n_splits}, Test Size={test_size}\")\n",
    "    print(f\"-\" * 40)\n",
    "    \n",
    "    # 1. 원본 데이터 로드\n",
    "    df = load_and_fix_data(input_path, is_test=False)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"데이터 로드 실패로 작업을 중단합니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"데이터 로드 완료: {len(df)}개 행\")\n",
    "\n",
    "    # 출력 루트 폴더 생성 (pathlib mkdir 사용)\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2. [Test Set 분리]\n",
    "    print(f\"\\n[Step 1] Test Set 분리 (비율: {test_size})\")\n",
    "    dev_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        stratify=df['label'], \n",
    "        random_state=SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Test Set 저장\n",
    "    test_save_path = output_root / OUTPUT_TEST_FILENAME\n",
    "    test_df.to_csv(test_save_path, index=False)\n",
    "    \n",
    "    print(f\" -> Test Set 저장 완료: {test_save_path.name}\")\n",
    "    print(f\"    (Size: {len(test_df)}, Label 0: {(test_df['label']==0).sum()}, Label 1: {(test_df['label']==1).sum()})\")\n",
    "\n",
    "    # 3. [K-Fold 분할 및 폴더별 저장]\n",
    "    print(f\"\\n[Step 2] {n_splits}-Fold Cross Validation 데이터 생성\")\n",
    "    \n",
    "    dev_df = dev_df.reset_index(drop=True)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(dev_df, dev_df['label'])):\n",
    "        fold_train_df = dev_df.iloc[train_idx]\n",
    "        fold_val_df = dev_df.iloc[val_idx]\n",
    "        \n",
    "        # 폴더 경로 생성 (fold0, fold1 ...)\n",
    "        fold_dir_name = f'fold{fold}'\n",
    "        current_fold_dir = output_root / fold_dir_name\n",
    "        \n",
    "        # 폴더 생성\n",
    "        current_fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 파일 저장\n",
    "        train_save_path = current_fold_dir / OUTPUT_TRAIN_FILENAME\n",
    "        val_save_path = current_fold_dir / OUTPUT_VALID_FILENAME\n",
    "        \n",
    "        fold_train_df.to_csv(train_save_path, index=False)\n",
    "        fold_val_df.to_csv(val_save_path, index=False)\n",
    "        \n",
    "        print(f\" -> [{fold_dir_name}] 저장 완료\")\n",
    "        # 상세 정보는 너무 길어질 수 있으니 필요하면 주석 해제\n",
    "        # print(f\"      Train: {len(fold_train_df)}개, Valid: {len(fold_val_df)}개\")\n",
    "\n",
    "    print(\"\\n>>> 모든 작업이 완료됨.\")\n",
    "    print(f\"최종 결과물 위치: {output_root}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실행 경로 조합\n",
    "    input_full_path = INPUT_DIR / INPUT_FILE_NAME\n",
    "    \n",
    "    create_split_and_kfold(\n",
    "        input_path=input_full_path, \n",
    "        output_root=OUTPUT_ROOT_DIR, \n",
    "        n_splits=4, \n",
    "        test_size=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cccad7",
   "metadata": {},
   "source": [
    "ai특징\n",
    "\n",
    "문단별로 ~다. ~임. ~함. (격식체 오류)\n",
    "이에 따라, 또한(논리적 오류)\n",
    "~등 다양한, ~에 대한(설명 오류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0645f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 데이터 로드 중... (/home/user/rocm_project/make-model/data/fold/fold0/origin_train.csv)\n",
      "🚀 어미 패턴 분석 시작 (시간이 조금 걸릴 수 있습니다)...\n",
      "\n",
      "==================================================\n",
      "📊 [분석 결과] '다/임/함' 말투 비율에 따른 통계\n",
      "==================================================\n",
      "             Sample Count  Actual AI Ratio\n",
      "ratio_group                               \n",
      "0~10%                  38         0.000000\n",
      "10~30%                 53         0.018868\n",
      "30~50%                143         0.195804\n",
      "50~70%               1014         0.297830\n",
      "70~90%               7774         0.304605\n",
      "90~100%             56568         0.047677\n",
      "\n",
      "📈 [해석]\n",
      " - '다/임/함' 도배된 글(90%↑)의 실제 AI 비율: 4.8%\n",
      " - 그런 말투가 없는 글(10%↓)의 실제 AI 비율: 0.0%\n",
      "🚨 중요 발견: '설명문 말투'를 쓰는 데이터의 대다수는 '사람(0)'입니다!\n",
      "   -> 모델이 이 말투를 AI로 착각하고 있다면, 이것이 가장 큰 오답 원인입니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = '/home/user/rocm_project/make-model/data/fold/fold0/origin_train.csv'\n",
    "\n",
    "# ==========================================\n",
    "# 어미 분석 함수\n",
    "# ==========================================\n",
    "def analyze_style(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "    \n",
    "    # 문장 분리 (. ? ! 기준으로 나눔)\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 1]\n",
    "    \n",
    "    if not sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    # 특정 어미로 끝나는 문장 카운트\n",
    "    # '다.', '다' / '임.', '임' / '함.', '함' 등을 모두 체크\n",
    "    pattern_count = 0\n",
    "    for s in sentences:\n",
    "        if s.endswith(('다.', '다', '임.', '임', '함.', '함')):\n",
    "            pattern_count += 1\n",
    "            \n",
    "    # 전체 문장 중 해당 패턴의 비율 (0.0 ~ 1.0)\n",
    "    return pattern_count / len(sentences)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 분석 실행\n",
    "# ==========================================\n",
    "def run_analysis():\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        print(f\"파일을 찾을 수 없습니다: {FILE_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"데이터 로드 중... ({FILE_PATH})\")\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "    \n",
    "    # 텍스트 컬럼 찾기\n",
    "    if 'text' not in df.columns:\n",
    "        # 유사 컬럼명 검색\n",
    "        for col in df.columns:\n",
    "            if col in ['paragraph_text', 'content', 'full_text']:\n",
    "                df.rename(columns={col: 'text'}, inplace=True)\n",
    "                break\n",
    "    \n",
    "    print(\"🚀 어미 패턴 분석 시작 (시간이 조금 걸릴 수 있습니다)...\")\n",
    "    # 'style_ratio': 전체 문장 중 '다/임/함'으로 끝나는 문장의 비율\n",
    "    df['style_ratio'] = df['text'].apply(analyze_style)\n",
    "    \n",
    "    # 구간(Bin) 나누기: 10% 단위로 구간화\n",
    "    # 예: 0.0~0.1, 0.1~0.2, ... 0.9~1.0\n",
    "    bins = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.01]\n",
    "    labels = ['0~10%', '10~30%', '30~50%', '50~70%', '70~90%', '90~100%']\n",
    "    df['ratio_group'] = pd.cut(df['style_ratio'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"[분석 결과] '다/임/함' 말투 비율에 따른 통계\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 그룹별 통계 계산\n",
    "    # 'label'이 있으면 실제 AI 비율을, 'prob_1'이 있으면 모델 예측 확률을 계산\n",
    "    agg_dict = {'text': 'count'} # 샘플 수\n",
    "    \n",
    "    if 'label' in df.columns:\n",
    "        agg_dict['label'] = 'mean' # 실제 AI 비율 (Ground Truth)\n",
    "        \n",
    "    if 'prob_1' in df.columns: # 예측 결과 파일인 경우\n",
    "        agg_dict['prob_1'] = 'mean' # 모델이 예측한 AI 확률\n",
    "    elif 'ensemble_prob_1' in df.columns:\n",
    "        agg_dict['ensemble_prob_1'] = 'mean'\n",
    "\n",
    "    grouped = df.groupby('ratio_group', observed=False).agg(agg_dict)\n",
    "    \n",
    "    # 컬럼 이름 보기 좋게 변경\n",
    "    rename_map = {'text': 'Sample Count'}\n",
    "    if 'label' in agg_dict: rename_map['label'] = 'Actual AI Ratio'\n",
    "    if 'prob_1' in agg_dict: rename_map['prob_1'] = 'Predicted AI Prob'\n",
    "    if 'ensemble_prob_1' in agg_dict: rename_map['ensemble_prob_1'] = 'Predicted AI Prob'\n",
    "    \n",
    "    grouped = grouped.rename(columns=rename_map)\n",
    "    print(grouped)\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 시각화 (선택 사항)\n",
    "    # -------------------------------------------------------\n",
    "    # Actual AI Ratio가 존재하면 그래프를 그림\n",
    "    if 'Actual AI Ratio' in grouped.columns:\n",
    "        print(\"\\n[해석]\")\n",
    "        high_ratio_group = grouped.loc['90~100%', 'Actual AI Ratio']\n",
    "        low_ratio_group = grouped.loc['0~10%', 'Actual AI Ratio']\n",
    "        \n",
    "        print(f\" - '다/임/함' 도배된 글(90%↑)의 실제 AI 비율: {high_ratio_group*100:.1f}%\")\n",
    "        print(f\" - 그런 말투가 없는 글(10%↓)의 실제 AI 비율: {low_ratio_group*100:.1f}%\")\n",
    "        \n",
    "        if high_ratio_group < 0.2: # 예: 설명문인데 AI 비율이 낮다면\n",
    "            print(\"result: '설명문 말투'를 쓰는 데이터의 대다수는 0\")\n",
    "        elif high_ratio_group > 0.8:\n",
    "            print(\"result: 이 데이터셋에서는 '설명문 말투'는 대부분 AI가 쓴 것이 맞음\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52825ae5",
   "metadata": {},
   "source": [
    "데이터 축소 작업\n",
    "Base: clean_train.csv\n",
    "method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18370b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중... /home/user/rocm_project/make-model/data/interim/clean_train.csv\n",
      "   원본 컬럼 목록: ['title', 'full_text', 'generated']\n",
      "   텍스트 컬럼 확인: 'full_text' -> 'text'로 변경\n",
      "   ✅ 라벨 컬럼 확인: 'generated' -> 'label'로 변경\n",
      "[Downsampling] 시작\n",
      "   원본 데이터: 97172개\n",
      "   - Label 0 (Human): 89177\n",
      "   - Label 1 (AI):    7995\n",
      "   목표: Human 데이터를 89177 -> 23985개로 축소 (비율 3.0:1)\n",
      "   ⏳ 중요도(Style Score) 계산 중...\n",
      "   ✅ 축소 완료: 31980개\n",
      "   - Label 0 평균 Style 점수: 0.975 -> 0.989 (어려운 데이터 위주로 남김)\n",
      "💾 저장 완료: /home/user/rocm_project/make-model/data/interim/clean_train_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# 1. 설정\n",
    "# ==========================================\n",
    "# 원본 학습 데이터 경로 (본인의 경로에 맞게 수정 확인)\n",
    "INPUT_PATH = '/home/user/rocm_project/make-model/data/interim/clean_train.csv'\n",
    "OUTPUT_PATH = '/home/user/rocm_project/make-model/data/interim/clean_train_balanced.csv'\n",
    "\n",
    "# 목표 비율 (Human : AI = 3 : 1)\n",
    "TARGET_RATIO = 3.0  \n",
    "\n",
    "# ==========================================\n",
    "# 2. 유틸리티 함수\n",
    "# ==========================================\n",
    "def analyze_style_score(text):\n",
    "\n",
    "    if not isinstance(text, str): return 0.0\n",
    "    \n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 1]\n",
    "    \n",
    "    if not sentences: return 0.0\n",
    "    \n",
    "    # AI스러운 어미 패턴\n",
    "    ai_like_count = 0\n",
    "    for s in sentences:\n",
    "        # 설명문 말투 (~다, ~임, ~함)\n",
    "        if s.endswith(('다.', '다', '임.', '임', '함.', '함')):\n",
    "            ai_like_count += 1\n",
    "        # 논리적 연결어 (가중치 추가)\n",
    "        if any(x in s for x in ['따라서', '또한', '이에', '즉', '의미한다']):\n",
    "            ai_like_count += 0.5 \n",
    "            \n",
    "    return min(1.0, ai_like_count / len(sentences))\n",
    "\n",
    "def smart_downsampling(df):\n",
    "    print(f\"[Downsampling] 시작\")\n",
    "    print(f\"   원본 데이터: {len(df)}개\")\n",
    "    print(f\"   - Label 0 (Human): {len(df[df['label']==0])}\")\n",
    "    print(f\"   - Label 1 (AI):    {len(df[df['label']==1])}\")\n",
    "    \n",
    "    # 1. 클래스 분리\n",
    "    df_human = df[df['label'] == 0].copy()\n",
    "    df_ai = df[df['label'] == 1].copy()\n",
    "    \n",
    "    # 2. 목표 개수 계산\n",
    "    n_ai = len(df_ai)\n",
    "    target_human_count = int(n_ai * TARGET_RATIO)\n",
    "    \n",
    "    if len(df_human) <= target_human_count:\n",
    "        print(\"   이미 비율이 목표보다 좋거나 데이터가 적어서 축소하지 않습니다.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"   목표: Human 데이터를 {len(df_human)} -> {target_human_count}개로 축소 (비율 {TARGET_RATIO}:1)\")\n",
    "    \n",
    "    # 3. 중요도 점수 계산\n",
    "    print(\"   중요도(Style Score) 계산 중...\")\n",
    "    df_human['style_score'] = df_human['text'].apply(analyze_style_score)\n",
    "    \n",
    "    # 4. 데이터 선별 전략\n",
    "    # - Top Tier (상위 60%): 설명문 스타일 -> 유지 (Hard Negative)\n",
    "    # - Bottom Tier (하위 40%): 대화체/쉬운글 -> 랜덤 삭제 (Easy Negative)\n",
    "    \n",
    "    df_human = df_human.sort_values(by='style_score', ascending=False)\n",
    "    \n",
    "    keep_n_high = int(target_human_count * 0.6) \n",
    "    keep_n_low = target_human_count - keep_n_high\n",
    "    \n",
    "    high_score_group = df_human.iloc[:keep_n_high]\n",
    "    low_score_group = df_human.iloc[keep_n_high:]\n",
    "    \n",
    "    # 하위 그룹에서 랜덤 샘플링\n",
    "    if len(low_score_group) > keep_n_low:\n",
    "        low_score_sampled = low_score_group.sample(n=keep_n_low, random_state=42)\n",
    "    else:\n",
    "        low_score_sampled = low_score_group\n",
    "        \n",
    "    # 합치기\n",
    "    df_human_balanced = pd.concat([high_score_group, low_score_sampled])\n",
    "    \n",
    "    # 최종 데이터셋 생성\n",
    "    final_df = pd.concat([df_human_balanced, df_ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # 결과 리포트\n",
    "    avg_score_before = df_human['style_score'].mean()\n",
    "    avg_score_after = df_human_balanced['style_score'].mean()\n",
    "    \n",
    "    print(f\"   축소 완료: {len(final_df)}개\")\n",
    "    print(f\"   - Label 0 평균 Style 점수: {avg_score_before:.3f} -> {avg_score_after:.3f} (어려운 데이터 위주로 남김)\")\n",
    "    \n",
    "    if 'style_score' in final_df.columns:\n",
    "        del final_df['style_score']\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "# ==========================================\n",
    "# 3. 메인 실행 (수정됨: 강력한 컬럼 찾기)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    if not Path(INPUT_PATH).exists():\n",
    "        print(f\"입력 파일이 없습니다: {INPUT_PATH}\")\n",
    "        sys.exit()\n",
    "\n",
    "    print(f\"데이터 로드 중... {INPUT_PATH}\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_PATH)\n",
    "    except:\n",
    "        df = pd.read_csv(INPUT_PATH, encoding='cp949')\n",
    "\n",
    "    print(f\"   원본 컬럼 목록: {list(df.columns)}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [수정] 텍스트 컬럼 자동 매칭 (더 많은 후보 추가)\n",
    "    # ---------------------------------------------------------\n",
    "    text_col = None\n",
    "    text_candidates = ['text', 'paragraph_text', 'content', 'sentence', 'full_text', 'overview', 'document']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.lower() in text_candidates:\n",
    "            text_col = col\n",
    "            break\n",
    "            \n",
    "    # 후보군에 없으면 object 타입 첫 번째 컬럼 선택\n",
    "    if text_col is None:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            text_col = obj_cols[0]\n",
    "\n",
    "    if text_col:\n",
    "        print(f\"   텍스트 컬럼 확인: '{text_col}' -> 'text'로 변경\")\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        print(\"[Error] 텍스트 컬럼을 찾을 수 없습니다. CSV 헤더를 확인해주세요.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # [수정] 라벨 컬럼 자동 매칭\n",
    "    # ---------------------------------------------------------\n",
    "    label_col = None\n",
    "    label_candidates = ['label', 'generated', 'target', 'class', 'answer']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col.lower() in label_candidates:\n",
    "            label_col = col\n",
    "            break\n",
    "            \n",
    "    if label_col:\n",
    "        print(f\"   ✅ 라벨 컬럼 확인: '{label_col}' -> 'label'로 변경\")\n",
    "        df.rename(columns={label_col: 'label'}, inplace=True)\n",
    "    else:\n",
    "        print(\"[Error] 라벨(정답) 컬럼을 찾을 수 없습니다.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 실행\n",
    "    # ---------------------------------------------------------\n",
    "    balanced_df = smart_downsampling(df)\n",
    "    \n",
    "    # 저장 폴더가 없으면 생성\n",
    "    Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    balanced_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"저장 완료: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
