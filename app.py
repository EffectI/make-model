import streamlit as st
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import time
import easyocr
from PIL import Image

# ==========================================
# 1. ê¸°ë³¸ ì„¤ì • ë° ìƒìˆ˜
# ==========================================
st.set_page_config(
    page_title="On-premise AI Detector",
    page_icon="ğŸ›¡ï¸",
    layout="wide"
)

# ëª¨ë¸ ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
MODEL_PATH = "experiments/koelectra_small_sliding_single_test"

# ==========================================
# 2. ë¦¬ì†ŒìŠ¤ ë¡œë“œ í•¨ìˆ˜ (Caching ì ìš©)
# ==========================================

# 2-1. KoELECTRA ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model_and_tokenizer(model_path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.to(device)
        model.eval()
        return tokenizer, model, device
    except Exception as e:
        print(f"Error loading model: {e}")
        return None, None, None

# 2-2. EasyOCR ëª¨ë¸ ë¡œë“œ (Vision ê¸°ëŠ¥ìš©)
@st.cache_resource
def load_ocr_reader():
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    use_gpu = torch.cuda.is_available()
    print(f"OCR Loading... GPU: {use_gpu}")
    # í•œêµ­ì–´(ko), ì˜ì–´(en) ì¸ì‹ ëª¨ë¸ ë¡œë“œ
    return easyocr.Reader(['ko', 'en'], gpu=use_gpu)

# ì „ì—­ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
tokenizer, model, device = load_model_and_tokenizer(MODEL_PATH)
ocr_reader = load_ocr_reader()

# ==========================================
# 3. ì‹¤ì œ ì¶”ë¡  í•¨ìˆ˜ (Core Logic)
# ==========================================
def predict_text(text, tokenizer, model, device):
    if not text or model is None:
        return 0.0, 0.0, 0.0

    start_time = time.time()
    
    # í† í°í™” ë° í…ì„œ ë³€í™˜
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length"
    ).to(device)

    # ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    
    # í™•ë¥  ê³„ì‚° (Softmax)
    probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
    
    end_time = time.time()
    latency = round((end_time - start_time) * 1000, 2) # ms ë‹¨ìœ„

    human_prob = probs[0]
    ai_prob = probs[1]

    return human_prob, ai_prob, latency

# ==========================================
# 4. í˜ì´ì§€ë³„ UI í•¨ìˆ˜
# ==========================================

def page_home():
    st.title("ğŸ›¡ï¸ On-premise AI Text Detector")
    st.markdown("### ê³ ì„±ëŠ¥ ê²½ëŸ‰í™” ëª¨ë¸ ê¸°ë°˜ì˜ í…ìŠ¤íŠ¸ ë¶„ì„ ì†”ë£¨ì…˜")
    
    st.divider()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.info("### ğŸ”’ Privacy First\në‚´ë¶€ ì„œë²„ êµ¬ë™ìœ¼ë¡œ ë°ì´í„° ìœ ì¶œ ê±±ì • ì—†ìŒ")
    with col2:
        st.success("### ğŸ’¸ Cost Efficiency\nAPI ë¹„ìš© ì—†ëŠ” ì €ë ´í•œ ìœ ì§€ë¹„ìš©")
    with col3:
        st.warning("### ğŸš€ High Performance\nKoELECTRA ëª¨ë¸ ê¸°ë°˜ ì •ë°€ ë¶„ì„")

    st.divider()
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
    st.subheader("System Status")
    c1, c2 = st.columns(2)
    with c1:
        if device and device.type == 'cuda':
            st.success(f"Running on GPU: {torch.cuda.get_device_name(0)}")
        else:
            st.warning("Running on CPU")
    with c2:
        if ocr_reader:
            st.success("OCR Engine: Ready")
        else:
            st.error("OCR Engine: Failed")

def page_lms():
    st.header("ğŸ“ LMS íƒ‘ì¬í˜• ê³¼ì œ ê²€ìˆ˜ê¸°")
    st.markdown("**ì—ì„¸ì´ë¥¼ ë¶„ì„í•˜ì—¬ AI ì‘ì„± ì—¬ë¶€ë¥¼ ìŠ¤í¬ë¦¬ë‹í•©ë‹ˆë‹¤.**")
    
    if model is None:
        st.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    with st.form("lms_form"):
        text = st.text_area("ê³¼ì œ ë‚´ìš© ì…ë ¥", height=300, placeholder="í•™ìƒì´ ì œì¶œí•œ ì—ì„¸ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        submitted = st.form_submit_button("ê²€ì‚¬ ìˆ˜í–‰")
        
    if submitted and text:
        with st.spinner("Deep Learning Model Analyzing..."):
            human_prob, ai_prob, latency = predict_text(text, tokenizer, model, device)
            word_count = len(text.split())
            
        # ê²°ê³¼ íŒì • ë¡œì§
        if ai_prob >= 0.85:
            status = "red_flag"
            label_msg = "High Risk (AI ì˜ì‹¬)"
        elif ai_prob >= 0.50:
            status = "warning"
            label_msg = "Medium Risk (ê²€í†  í•„ìš”)"
        else:
            status = "clear"
            label_msg = "Low Risk (ì‚¬ëŒ ì‘ì„±)"
            
        st.divider()
        c1, c2 = st.columns([1, 2])
        with c1:
            st.metric("ë‹¨ì–´ ìˆ˜", f"{word_count} words")
            st.metric("AI í™•ë¥ ", f"{ai_prob*100:.1f}%")
            st.caption(f"Latency: {latency}ms")
            
        with c2:
            if status == "red_flag":
                st.error(f"ğŸš© **RED FLAG ê°ì§€ë¨**\n\nAI ì‘ì„± íŒ¨í„´ì´ ê°•í•˜ê²Œ ì˜ì‹¬ë©ë‹ˆë‹¤ ({label_msg}).\n**ì •ë°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.**")
            elif status == "warning":
                st.warning(f"âš ï¸ **ì£¼ì˜ ìš”ë§**\n\nì¼ë¶€ ë¬¸ì¥ì´ ë¶€ìì—°ìŠ¤ëŸ½ê±°ë‚˜ AI íŒ¨í„´ì´ ì„ì—¬ìˆìŠµë‹ˆë‹¤ ({label_msg}).")
            else:
                st.success(f"âœ… **í†µê³¼ (Clear)**\n\nì‚¬ëŒì´ ì‘ì„±í•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤ ({label_msg}).")
            
            st.write("#### ìƒì„¸ ë¶„ì„")
            st.progress(int(ai_prob * 100), text=f"AI Score: {ai_prob*100:.1f}%")
            st.progress(int(human_prob * 100), text=f"Human Score: {human_prob*100:.1f}%")

def page_spam():
    st.header("ğŸš¨ ì‹¤ì‹œê°„ ìŠ¤íŒ¸/í”¼ì‹± íƒì§€ê¸° (Vision)")
    st.markdown("**ë¬¸ì ë‚´ìš©ì„ ì…ë ¥í•˜ê±°ë‚˜, ìŠ¤í¬ë¦°ìƒ·/ì¹´ë©”ë¼ë¡œ ì°ìœ¼ë©´ ì¦‰ì‹œ ë¶„ì„í•©ë‹ˆë‹¤.**")
    
    if model is None:
        st.error("ëª¨ë¸ ì˜¤ë¥˜: ë¡œë“œ ì‹¤íŒ¨")
        return

    # íƒ­ìœ¼ë¡œ ì…ë ¥ ë°©ì‹ ë¶„ë¦¬
    tab1, tab2 = st.tabs(["ğŸ“ í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥", "ğŸ“¸ ìŠ¤í¬ë¦°ìƒ·/ì¹´ë©”ë¼ ë¶„ì„"])
    
    target_text = ""
    is_image_processed = False

    # [Tab 1] í…ìŠ¤íŠ¸ ì…ë ¥
    with tab1:
        user_input = st.text_area("ë©”ì‹œì§€ ë‚´ìš©", height=150, placeholder="[Webë°œì‹ ] ë‹¹ì²¨ì„ ì¶•í•˜í•©ë‹ˆë‹¤...", key="text_input_area")
        if st.button("í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰", key="btn_text"):
            target_text = user_input

    # [Tab 2] OCR (ì´ë¯¸ì§€ ë¶„ì„) - PC ë¶™ì—¬ë„£ê¸° ê°€ì´ë“œ ì¶”ê°€ë¨
    with tab2:
        # PC ì‚¬ìš©ììš© ê°€ì´ë“œ
        st.info("""
        ğŸ’¡ **PC ì‚¬ìš©ì ìº¡ì²˜ íŒ:**
        1. **`Win` + `Shift` + `S`** í‚¤ë¡œ ìº¡ì²˜
        2. ì•„ë˜ **'Browse files'** ì˜ì—­ í´ë¦­
        3. **`Ctrl` + `V`** ë¡œ ë¶™ì—¬ë„£ê¸°
        """)
        
        # íŒŒì¼ ì—…ë¡œë”ì™€ ì¹´ë©”ë¼ ì…ë ¥ì„ ë™ì‹œì— ì§€ì›
        col_img1, col_img2 = st.columns(2)
        with col_img1:
            # ë¼ë²¨ì— (Ctrl+V) ëª…ì‹œ
            img_file = st.file_uploader("ì´ë¯¸ì§€ ë¶™ì—¬ë„£ê¸°(Ctrl+V) ë˜ëŠ” ì—…ë¡œë“œ", type=['png', 'jpg', 'jpeg'])
        with col_img2:
            camera_input = st.camera_input("ì¹´ë©”ë¼ë¡œ ì°ê¸° (ëª¨ë°”ì¼)")
        
        # ìš°ì„ ìˆœìœ„: ì¹´ë©”ë¼ -> íŒŒì¼ ì—…ë¡œë“œ
        target_image = camera_input if camera_input else img_file
        
        if target_image:
            # ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°
            st.image(target_image, caption="ë¶„ì„ ëŒ€ìƒ ì´ë¯¸ì§€", width=400)
            
            if st.button("ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘", type="primary", key="btn_ocr"):
                with st.spinner("ğŸ“· ì´ë¯¸ì§€ì—ì„œ ê¸€ìë¥¼ ì½ì–´ë‚´ëŠ” ì¤‘... (OCR)"):
                    try:
                        image = Image.open(target_image)
                        image_np = np.array(image)
                        
                        # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (detail=0ì€ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜)
                        result = ocr_reader.readtext(image_np, detail=0)
                        target_text = " ".join(result)
                        is_image_processed = True
                        
                        if not target_text.strip():
                            st.warning("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # [ê³µí†µ] ë¶„ì„ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥
    if target_text:
        st.divider()
        if is_image_processed:
            with st.expander("ğŸ” ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë³´ê¸°", expanded=True):
                st.text(target_text)
        
        with st.spinner("ğŸ¤– AI ëª¨ë¸ì´ ìŠ¤íŒ¸ ì—¬ë¶€ë¥¼ íŒë‹¨ ì¤‘ì…ë‹ˆë‹¤..."):
            human_prob, ai_prob, latency = predict_text(target_text, tokenizer, model, device)

        # ê²°ê³¼ ì¹´ë“œ ë””ìì¸
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.metric("AI ìŠ¤íŒ¸ í™•ë¥ ", f"{ai_prob*100:.1f}%")
            
        with col2:
            if ai_prob >= 0.85:
                st.error(f"ğŸš« **ìœ„í—˜ (DANGER)**\n\ní”¼ì‹±/ìŠ¤íŒ¸ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!")
            elif ai_prob >= 0.50:
                st.warning(f"âš ï¸ **ì£¼ì˜ (WARNING)**\n\nì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.success(f"âœ… **ì•ˆì „ (SAFE)**\n\nì •ìƒì ì¸ ë©”ì‹œì§€ë¡œ ë³´ì…ë‹ˆë‹¤.")
        
        st.caption(f"ğŸ“Š ë¶„ì„ ëª¨ë¸: KoELECTRA Custom | âš¡ Latency: **{latency}ms**")
        
        with st.expander("ê°œë°œììš© ë””ë²„ê·¸ ì •ë³´"):
            st.json({
                "source_length": len(target_text),
                "human_prob": float(human_prob),
                "ai_prob": float(ai_prob),
                "ocr_used": is_image_processed
            })

# ==========================================
# 5. ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬
# ==========================================
def main():
    with st.sidebar:
        st.title("ğŸ”§ ì†”ë£¨ì…˜ ëª¨ë“œ ì„ íƒ")
        choice = st.radio("Mode", ["í”„ë¡œì íŠ¸ ì†Œê°œ", "LMS ê³¼ì œ ê²€ìˆ˜", "ì‹¤ì‹œê°„ ìŠ¤íŒ¸ íƒì§€"])
        
        st.divider()
        st.markdown("### System Info")
        if device:
            st.caption(f"Device: {device}")
            st.caption(f"Model: {MODEL_PATH.split('/')[-1]}")
        else:
            st.error("Model Load Failed")
        
        st.markdown("---")
        st.caption("Developed with Streamlit & EasyOCR")

    if choice == "í”„ë¡œì íŠ¸ ì†Œê°œ":
        page_home()
    elif choice == "LMS ê³¼ì œ ê²€ìˆ˜":
        page_lms()
    elif choice == "ì‹¤ì‹œê°„ ìŠ¤íŒ¸ íƒì§€":
        page_spam()

if __name__ == "__main__":
    main()