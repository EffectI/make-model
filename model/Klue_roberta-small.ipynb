{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a71e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "import csv\n",
    "import random\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# [1] 설정 및 하이퍼파라미터\n",
    "# =========================\n",
    "\n",
    "# 1. 경로 설정\n",
    "DATA_ROOT_DIR = 'make-model/data/fold'\n",
    "OUTPUT_ROOT_DIR = 'make-model/model/result_models/klue_small'\n",
    "\n",
    "# 2. 파일명 설정\n",
    "TEST_FILENAME = 'local_origin_test.csv'\n",
    "TRAIN_FILENAME = 'origin_train.csv'\n",
    "VALID_FILENAME = 'origin_valid.csv'\n",
    "\n",
    "# 3. 폴드 설정\n",
    "N_FOLDS = 4\n",
    "FOLD_DIR_PREFIX = 'fold'\n",
    "\n",
    "# 4. 모델 및 학습 설정\n",
    "MODEL_NAME = \"klue/roberta-small\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-5\n",
    "PATIENCE = 3\n",
    "SEED = 42\n",
    "\n",
    "detected_delimiter = ','\n",
    "detected_quotechar = '\"'\n",
    "\n",
    "# ==========================================\n",
    "# [2] 유틸리티 함수\n",
    "# ==========================================\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "def find_column_name(columns, candidates):\n",
    "    for col in columns:\n",
    "        if col.lower().strip() in candidates:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_fix_data(path, is_test=False):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"파일이 없습니다: {path}\")\n",
    "        return None\n",
    "\n",
    "    df = None\n",
    "    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp949']\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                encoding=encoding,\n",
    "                engine='python',\n",
    "                on_bad_lines='skip',\n",
    "                encoding_errors='ignore',\n",
    "                delimiter=detected_delimiter,\n",
    "                quotechar=detected_quotechar,\n",
    "                quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        print(f\"데이터 로드 실패: {path}\")\n",
    "        return None\n",
    "\n",
    "    text_candidates = ['paragraph_text', 'text', 'sentence', 'content', 'full_text']\n",
    "    text_col = find_column_name(df.columns, text_candidates)\n",
    "    if text_col:\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            df.rename(columns={obj_cols[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if is_test:\n",
    "        id_candidates = ['id', 'idx', 'index', 'no', 'ID']\n",
    "        id_col = find_column_name(df.columns, id_candidates)\n",
    "        if id_col:\n",
    "            df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "        else:\n",
    "            df['id'] = df.index\n",
    "\n",
    "    if not is_test:\n",
    "        target_candidates = ['generated', 'label', 'target', 'class']\n",
    "        target_col = find_column_name(df.columns, target_candidates)\n",
    "        if target_col:\n",
    "            df.rename(columns={target_col: 'label'}, inplace=True)\n",
    "            try:\n",
    "                df['label'] = df['label'].astype(int)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"Target(Label) 컬럼을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# ==========================================\n",
    "# [3] 메인 학습 루프 (K-Fold)\n",
    "# ==========================================\n",
    "\n",
    "def run_kfold_process():\n",
    "    set_seeds(SEED)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n[{MODEL_NAME}] {N_FOLDS}-Fold 학습 시작\")\n",
    "    print(f\"Data Root: {DATA_ROOT_DIR}\")\n",
    "    print(f\"Output Root: {OUTPUT_ROOT_DIR}\")\n",
    "\n",
    "    # Test 데이터 로드\n",
    "    test_file_path = os.path.join(DATA_ROOT_DIR, TEST_FILENAME)\n",
    "    test_df = load_and_fix_data(test_file_path, is_test=False)\n",
    "    \n",
    "    if test_df is None:\n",
    "        print(\"Test Set 로드 실패. 경로를 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Common Test Set Loaded: {len(test_df)} samples\")\n",
    "    test_ds = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "    encoded_test = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    # ==========================\n",
    "    # Loop over Folds\n",
    "    # ==========================\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\" >>> [FOLD {fold_idx}] Start Training\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "        current_fold_dir = os.path.join(DATA_ROOT_DIR, f\"{FOLD_DIR_PREFIX}{fold_idx}\")\n",
    "        train_path = os.path.join(current_fold_dir, TRAIN_FILENAME)\n",
    "        val_path = os.path.join(current_fold_dir, VALID_FILENAME)\n",
    "        fold_output_dir = os.path.join(OUTPUT_ROOT_DIR, f\"{FOLD_DIR_PREFIX}{fold_idx}\")\n",
    "\n",
    "        print(f\" - Train: {train_path}\")\n",
    "        print(f\" - Valid: {val_path}\")\n",
    "        print(f\" - Output: {fold_output_dir}\")\n",
    "\n",
    "        train_df = load_and_fix_data(train_path)\n",
    "        val_df = load_and_fix_data(val_path)\n",
    "\n",
    "        if train_df is None or val_df is None:\n",
    "            print(f\"!! [Fold {fold_idx}] 데이터 로드 실패. 건너뜁니다.\")\n",
    "            continue\n",
    "\n",
    "        train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "        val_ds = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "\n",
    "        encoded_train = train_ds.map(preprocess, batched=True)\n",
    "        encoded_val = val_ds.map(preprocess, batched=True)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=fold_output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            weight_decay=0.01,\n",
    "            fp16=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            report_to=\"none\",\n",
    "            seed=SEED\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model, args=args,\n",
    "            train_dataset=encoded_train, eval_dataset=encoded_val,\n",
    "            tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(fold_output_dir)\n",
    "        tokenizer.save_pretrained(fold_output_dir)\n",
    "\n",
    "        # -----------------------\n",
    "        # 기본 평가 (Metric 계산)\n",
    "        # -----------------------\n",
    "        print(f\">>> [Fold {fold_idx}] Evaluating on Test Set...\")\n",
    "        metrics = trainer.evaluate(encoded_test)\n",
    "        print(f\"    Result: {metrics}\")\n",
    "        all_fold_metrics.append(metrics)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 상세 예측 결과 저장 (Correlation 분석용)\n",
    "        # ---------------------------------------\n",
    "        print(f\">>> [Fold {fold_idx}] Saving Predictions for Correlation Analysis...\")\n",
    "        \n",
    "        pred_output = trainer.predict(encoded_test)\n",
    "        logits = pred_output.predictions\n",
    "        \n",
    "        # Softmax를 적용하여 확률값(Probability) 추출\n",
    "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        \n",
    "        fold_pred_df = test_df.copy()\n",
    "        \n",
    "        # text 컬럼이 너무 길면 보기 불편하므로 제거 가능\n",
    "        # fold_pred_df = fold_pred_df.drop(columns=['text']) \n",
    "        \n",
    "        fold_pred_df['prob_0'] = probs[:, 0]  # Class 0일 확률\n",
    "        fold_pred_df['prob_1'] = probs[:, 1]  # Class 1일 확률\n",
    "        fold_pred_df['pred_label'] = np.argmax(probs, axis=1) # 최종 예측 라벨\n",
    "        \n",
    "        # CSV 저장 (예: .../result/fold0/fold0_predictions.csv)\n",
    "        pred_save_path = os.path.join(fold_output_dir, f\"{FOLD_DIR_PREFIX}{fold_idx}_predictions.csv\")\n",
    "        fold_pred_df.to_csv(pred_save_path, index=False)\n",
    "        print(f\"    Saved: {pred_save_path}\")\n",
    "\n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # ==========================\n",
    "    # Final Summary\n",
    "    # ==========================\n",
    "    print(\"\\n\" + \"#\"*50)\n",
    "    print(\" [K-Fold Training Summary]\")\n",
    "    print(\"#\"*50)\n",
    "\n",
    "    avg_acc = 0\n",
    "    avg_f1 = 0\n",
    "\n",
    "    for i, m in enumerate(all_fold_metrics):\n",
    "        print(f\" Fold {i} -> Accuracy: {m['eval_accuracy']:.4f}, F1: {m['eval_f1']:.4f}\")\n",
    "        avg_acc += m['eval_accuracy']\n",
    "        avg_f1 += m['eval_f1']\n",
    "\n",
    "    if len(all_fold_metrics) > 0:\n",
    "        print(\"-\" * 50)\n",
    "        print(f\" Average -> Accuracy: {avg_acc/len(all_fold_metrics):.4f}, F1: {avg_f1/len(all_fold_metrics):.4f}\")\n",
    "    print(\"#\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        run_kfold_process()\n",
    "    else:\n",
    "        print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a694c",
   "metadata": {},
   "source": [
    "**submission 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "\n",
    "BASE_DIR = 'make-model/model'\n",
    "DATA_DIR = 'make-model/data'\n",
    "SAVED_MODEL_NAME = 'klue_roberta_small_result'\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, SAVED_MODEL_NAME)\n",
    "save_csv_path = 'make-model/temp_submission.csv'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"모델 로드 경로: {OUTPUT_DIR}\")\n",
    "saved_model_path = OUTPUT_DIR\n",
    "test_data_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    print(f\"'{test_data_path}' 파일이 없음\")\n",
    "    test_data_path = '/test.csv'\n",
    "\n",
    "test_df = load_and_fix_data(test_data_path, is_test=True)\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"테스트 데이터 로드 성공: {len(test_df)}행\")\n",
    "    \n",
    "    try:\n",
    "        loaded_model = AutoModelForSequenceClassification.from_pretrained(saved_model_path)\n",
    "        loaded_tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "        print(\"모델 및 토크나이저 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        print(\"   -> 경로 및 모델 파일이 존재하는지 확인.\")\n",
    "        loaded_model = None\n",
    "\n",
    "    if loaded_model:\n",
    "        test_ds = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "        def token_func(examples):\n",
    "            return loaded_tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "        encoded_test = test_ds.map(token_func, batched=True)\n",
    "\n",
    "        temp_inference_dir = os.path.join(BASE_DIR, 'temp_inference')\n",
    "        \n",
    "        inference_args = TrainingArguments(\n",
    "            output_dir=temp_inference_dir,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            fp16=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        inference_trainer = Trainer(\n",
    "            model=loaded_model,\n",
    "            args=inference_args,\n",
    "            tokenizer=loaded_tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(loaded_tokenizer)\n",
    "        )\n",
    "\n",
    "        print(\">>> 예측 수행 중...\")\n",
    "        pred_output = inference_trainer.predict(encoded_test)\n",
    "\n",
    "        logits = torch.tensor(pred_output.predictions)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        prob_class_1 = probs[:, 1].numpy() \n",
    "\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': test_df['id'],  \n",
    "            'generated': prob_class_1\n",
    "        })\n",
    "\n",
    "        submission.to_csv(save_csv_path, index=False)\n",
    "\n",
    "        print(f\"\\n파일 생성 완료: {save_csv_path}\")\n",
    "        print(submission.head())\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"test 데이터 로드 실패\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
