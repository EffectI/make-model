{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a71e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Pyo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "No GPU detected. Training might be slow.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "import csv\n",
    "import random\n",
    "from pathlib import Path \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# [1] 설정 및 하이퍼파라미터 (Pathlib 적용)\n",
    "# ==========================================\n",
    "\n",
    "# 1. 경로 설정\n",
    "# 현재 작업 경로(make-model 폴더)를 기준으로 설정\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# 데이터 경로: 이전 단계(K-Fold 분할)에서 저장된 위치\n",
    "DATA_ROOT_DIR = PROJECT_ROOT / 'data' / 'fold'\n",
    "\n",
    "# 모델 저장 경로: 결과 모델이 저장될 위치\n",
    "OUTPUT_ROOT_DIR = PROJECT_ROOT / 'model' / 'result_models' / 'klue_small'\n",
    "\n",
    "# 2. 파일명 설정\n",
    "TEST_FILENAME = 'local_origin_test.csv'\n",
    "TRAIN_FILENAME = 'origin_train.csv'\n",
    "VALID_FILENAME = 'origin_valid.csv'\n",
    "\n",
    "# 3. 폴드 설정\n",
    "N_FOLDS = 4\n",
    "FOLD_DIR_PREFIX = 'fold'\n",
    "\n",
    "# 4. 모델 및 학습 설정\n",
    "MODEL_NAME = \"klue/roberta-small\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-5\n",
    "PATIENCE = 2\n",
    "SEED = 42\n",
    "\n",
    "detected_delimiter = ','\n",
    "detected_quotechar = '\"'\n",
    "\n",
    "# ==========================================\n",
    "# [2] 유틸리티 함수\n",
    "# ==========================================\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "def find_column_name(columns, candidates):\n",
    "    for col in columns:\n",
    "        if col.lower().strip() in candidates:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_fix_data(path, is_test=False):\n",
    "    # Path 객체 호환성 체크\n",
    "    if not path.exists():\n",
    "        print(f\"파일이 없습니다: {path}\")\n",
    "        return None\n",
    "\n",
    "    df = None\n",
    "    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp949']\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                encoding=encoding,\n",
    "                engine='python',\n",
    "                on_bad_lines='skip',\n",
    "                encoding_errors='ignore',\n",
    "                delimiter=detected_delimiter,\n",
    "                quotechar=detected_quotechar,\n",
    "                quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        print(f\"데이터 로드 실패: {path}\")\n",
    "        return None\n",
    "\n",
    "    text_candidates = ['paragraph_text', 'text', 'sentence', 'content', 'full_text']\n",
    "    text_col = find_column_name(df.columns, text_candidates)\n",
    "    if text_col:\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            df.rename(columns={obj_cols[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if is_test:\n",
    "        id_candidates = ['id', 'idx', 'index', 'no', 'ID']\n",
    "        id_col = find_column_name(df.columns, id_candidates)\n",
    "        if id_col:\n",
    "            df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "        else:\n",
    "            df['id'] = df.index\n",
    "\n",
    "    if not is_test:\n",
    "        target_candidates = ['generated', 'label', 'target', 'class']\n",
    "        target_col = find_column_name(df.columns, target_candidates)\n",
    "        if target_col:\n",
    "            df.rename(columns={target_col: 'label'}, inplace=True)\n",
    "            try:\n",
    "                df['label'] = df['label'].astype(int)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"Target(Label) 컬럼을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# ==========================================\n",
    "# [3] 메인 학습 루프 (K-Fold)\n",
    "# ==========================================\n",
    "\n",
    "def run_kfold_process():\n",
    "    set_seeds(SEED)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n[{MODEL_NAME}] {N_FOLDS}-Fold 학습 시작\")\n",
    "    print(f\"Data Root: {DATA_ROOT_DIR}\")\n",
    "    print(f\"Output Root: {OUTPUT_ROOT_DIR}\")\n",
    "\n",
    "    # Test 데이터 로드 (Pathlib 사용)\n",
    "    test_file_path = DATA_ROOT_DIR / TEST_FILENAME\n",
    "    test_df = load_and_fix_data(test_file_path, is_test=False)\n",
    "    \n",
    "    if test_df is None:\n",
    "        print(\"Test Set 로드 실패. 경로를 확인하세요.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Common Test Set Loaded: {len(test_df)} samples\")\n",
    "    test_ds = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "    encoded_test = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    # ==========================\n",
    "    # Loop over Folds\n",
    "    # ==========================\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\" >>> [FOLD {fold_idx}] Start Training\")\n",
    "        print(\"=\"*40)\n",
    "\n",
    "        # 경로 설정 (Pathlib의 / 연산자 활용)\n",
    "        current_fold_dir = DATA_ROOT_DIR / f\"{FOLD_DIR_PREFIX}{fold_idx}\"\n",
    "        train_path = current_fold_dir / TRAIN_FILENAME\n",
    "        val_path = current_fold_dir / VALID_FILENAME\n",
    "        \n",
    "        # 모델 저장 경로\n",
    "        fold_output_dir = OUTPUT_ROOT_DIR / f\"{FOLD_DIR_PREFIX}{fold_idx}\"\n",
    "\n",
    "        print(f\" - Train Path: {train_path}\")\n",
    "        print(f\" - Valid Path: {val_path}\")\n",
    "        print(f\" - Save Path : {fold_output_dir}\")\n",
    "\n",
    "        train_df = load_and_fix_data(train_path)\n",
    "        val_df = load_and_fix_data(val_path)\n",
    "\n",
    "        if train_df is None or val_df is None:\n",
    "            print(f\"!! [Fold {fold_idx}] 데이터 로드 실패. 건너뜁니다.\")\n",
    "            continue\n",
    "\n",
    "        train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "        val_ds = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "\n",
    "        encoded_train = train_ds.map(preprocess, batched=True)\n",
    "        encoded_val = val_ds.map(preprocess, batched=True)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=str(fold_output_dir), \n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            weight_decay=0.01,\n",
    "            fp16=True,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=1,\n",
    "            report_to=\"none\",\n",
    "            seed=SEED\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model, args=args,\n",
    "            train_dataset=encoded_train, eval_dataset=encoded_val,\n",
    "            tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        # 모델 저장\n",
    "        trainer.save_model(str(fold_output_dir))\n",
    "        tokenizer.save_pretrained(str(fold_output_dir))\n",
    "\n",
    "        # -----------------------\n",
    "        # 기본 평가 (Metric 계산)\n",
    "        # -----------------------\n",
    "        print(f\">>> [Fold {fold_idx}] Evaluating on Test Set...\")\n",
    "        metrics = trainer.evaluate(encoded_test)\n",
    "        print(f\"    Result: {metrics}\")\n",
    "        all_fold_metrics.append(metrics)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # 상세 예측 결과 저장 (Correlation 분석용)\n",
    "        # ---------------------------------------\n",
    "        print(f\">>> [Fold {fold_idx}] Saving Predictions for Correlation Analysis...\")\n",
    "        \n",
    "        pred_output = trainer.predict(encoded_test)\n",
    "        logits = pred_output.predictions\n",
    "        \n",
    "        # Softmax를 적용하여 확률값(Probability) 추출\n",
    "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        \n",
    "        fold_pred_df = test_df.copy()\n",
    "        \n",
    "        # text 컬럼이 너무 길면 보기 불편하므로 제거 가능\n",
    "        # fold_pred_df = fold_pred_df.drop(columns=['text']) \n",
    "        \n",
    "        fold_pred_df['prob_0'] = probs[:, 0]  # Class 0일 확률\n",
    "        fold_pred_df['prob_1'] = probs[:, 1]  # Class 1일 확률\n",
    "        fold_pred_df['pred_label'] = np.argmax(probs, axis=1) # 최종 예측 라벨\n",
    "        \n",
    "        # CSV 저장\n",
    "        pred_save_path = fold_output_dir / f\"{FOLD_DIR_PREFIX}{fold_idx}_predictions.csv\"\n",
    "        fold_pred_df.to_csv(pred_save_path, index=False)\n",
    "        print(f\"    Saved: {pred_save_path}\")\n",
    "\n",
    "        del model, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # ==========================\n",
    "    # Final Summary\n",
    "    # ==========================\n",
    "    print(\"\\n\" + \"#\"*50)\n",
    "    print(\" [K-Fold Training Summary]\")\n",
    "    print(\"#\"*50)\n",
    "\n",
    "    avg_acc = 0\n",
    "    avg_f1 = 0\n",
    "\n",
    "    for i, m in enumerate(all_fold_metrics):\n",
    "        print(f\" Fold {i} -> Accuracy: {m['eval_accuracy']:.4f}, F1: {m['eval_f1']:.4f}\")\n",
    "        avg_acc += m['eval_accuracy']\n",
    "        avg_f1 += m['eval_f1']\n",
    "\n",
    "    if len(all_fold_metrics) > 0:\n",
    "        print(\"-\" * 50)\n",
    "        print(f\" Average -> Accuracy: {avg_acc/len(all_fold_metrics):.4f}, F1: {avg_f1/len(all_fold_metrics):.4f}\")\n",
    "    print(\"#\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "        run_kfold_process()\n",
    "    else:\n",
    "        print(\"No GPU detected. Training might be slow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a694c",
   "metadata": {},
   "source": [
    "**submission 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "\n",
    "# ==========================================\n",
    "# 1. 경로 및 하이퍼파라미터 설정\n",
    "# ==========================================\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# 학습된 모델이 있는 경로 가장 좋았던 fold를 설정\n",
    "MODEL_DIR = PROJECT_ROOT / 'model' / 'result_models' / 'klue_small' / 'fold0'\n",
    "\n",
    "# 테스트 데이터 경로\n",
    "TEST_DATA_PATH = PROJECT_ROOT / 'data' / 'fold' / 'local_origin_test.csv'\n",
    "\n",
    "# 결과 제출 파일 저장 경로\n",
    "SAVE_CSV_PATH = PROJECT_ROOT / 'temp_submission.csv'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 64\n",
    "DETECTED_DELIMITER = ','\n",
    "DETECTED_QUOTECHAR = '\"'\n",
    "\n",
    "# ==========================================\n",
    "# 2. 데이터 로드 유틸리티 함수\n",
    "# ==========================================\n",
    "def find_column_name(columns, candidates):\n",
    "    for col in columns:\n",
    "        if col.lower().strip() in candidates:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_fix_data(path, is_test=True):\n",
    "    if not path.exists():\n",
    "        print(f\"파일이 없음: {path}\")\n",
    "        return None\n",
    "\n",
    "    df = None\n",
    "    encodings = ['utf-8-sig', 'utf-8', 'cp949']\n",
    "\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path, encoding=enc, engine='python', on_bad_lines='skip',\n",
    "                encoding_errors='ignore', delimiter=DETECTED_DELIMITER,\n",
    "                quotechar=DETECTED_QUOTECHAR, quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # 텍스트 컬럼 표준화\n",
    "    text_candidates = ['paragraph_text', 'text', 'sentence', 'content', 'full_text']\n",
    "    text_col = find_column_name(df.columns, text_candidates)\n",
    "    if text_col:\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            df.rename(columns={obj_cols[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # ID 컬럼 표준화 (Test 데이터 필수)\n",
    "    if is_test:\n",
    "        id_candidates = ['id', 'idx', 'index', 'no', 'ID']\n",
    "        id_col = find_column_name(df.columns, id_candidates)\n",
    "        if id_col:\n",
    "            df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "        else:\n",
    "            df['id'] = df.index\n",
    "\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. 메인 추론 로직\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"모델 로드 경로: {MODEL_DIR}\")\n",
    "    print(f\"테스트 데이터 경로: {TEST_DATA_PATH}\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    test_df = load_and_fix_data(TEST_DATA_PATH, is_test=True)\n",
    "\n",
    "    if test_df is None:\n",
    "        print(\"테스트 데이터 로드 실패\")\n",
    "        return\n",
    "\n",
    "    print(f\"테스트 데이터 로드 성공: {len(test_df)}행\")\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    try:\n",
    "        loaded_model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "        loaded_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "        print(\"모델 및 토크나이저 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        return\n",
    "\n",
    "    # 데이터셋 생성 및 토큰화\n",
    "    test_ds = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "    def token_func(examples):\n",
    "        return loaded_tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "    encoded_test = test_ds.map(token_func, batched=True)\n",
    "\n",
    "    # 추론 설정\n",
    "    temp_inference_dir = PROJECT_ROOT / 'model' / 'temp_inference'\n",
    "    \n",
    "    inference_args = TrainingArguments(\n",
    "        output_dir=str(temp_inference_dir),\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        fp16=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    inference_trainer = Trainer(\n",
    "        model=loaded_model,\n",
    "        args=inference_args,\n",
    "        tokenizer=loaded_tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(loaded_tokenizer)\n",
    "    )\n",
    "\n",
    "    # 예측 수행\n",
    "    print(\"예측 수행 중...\")\n",
    "    pred_output = inference_trainer.predict(encoded_test)\n",
    "\n",
    "    # 로짓을 확률로 변환\n",
    "    logits = torch.tensor(pred_output.predictions)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    prob_class_1 = probs[:, 1].numpy()\n",
    "\n",
    "    # 결과 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['id'],\n",
    "        'generated': prob_class_1\n",
    "    })\n",
    "\n",
    "    submission.to_csv(SAVE_CSV_PATH, index=False)\n",
    "    print(f\"파일 생성 완료: {SAVE_CSV_PATH}\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
