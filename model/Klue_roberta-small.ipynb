{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a71e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback \n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===========\n",
    "# [설정 변수]\n",
    "# ===========\n",
    "BASE_DIR = '/'\n",
    "DATA_DIR = '/'\n",
    "MODEL_NAME = \"klue/roberta-small\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'klue_roberta_small_result')\n",
    "TRAIN_FILE_NAME = 'train_fixed.csv'  \n",
    "VAL_FILE_NAME = 'valid_fixed.csv'      \n",
    "TEST_FILE_NAME = 'local_test_fixed.csv'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 128    \n",
    "EPOCHS = 10         \n",
    "LEARNING_RATE = 5e-5 \n",
    "PATIENCE = 3      \n",
    "SEED = 42           \n",
    "\n",
    "detected_delimiter = ','\n",
    "detected_quotechar = '\"'\n",
    "\n",
    "# ==============\n",
    "# [유틸리티 함수]\n",
    "# ==============\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"random seed set to {seed}\")\n",
    "\n",
    "def find_column_name(columns, candidates):\n",
    "    for col in columns:\n",
    "        if col.lower().strip() in candidates:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_fix_data(path, is_test=False):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"파일이 없습니다: {path}\")\n",
    "        return None\n",
    "\n",
    "    df = None\n",
    "    encodings_to_try = ['utf-8-sig', 'utf-8', 'cp949']\n",
    "\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                encoding=encoding,\n",
    "                engine='python',\n",
    "                on_bad_lines='skip',       \n",
    "                encoding_errors='ignore',  \n",
    "                delimiter=detected_delimiter,\n",
    "                quotechar=detected_quotechar,\n",
    "                quoting=csv.QUOTE_MINIMAL\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            df = None\n",
    "\n",
    "    if df is None:\n",
    "        print(f\"데이터 로드 실패: {path}\")\n",
    "        return None\n",
    "\n",
    "    text_candidates = ['paragraph_text', 'text', 'sentence', 'content', 'full_text']\n",
    "    text_col = find_column_name(df.columns, text_candidates)\n",
    "    if text_col:\n",
    "        df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    else:\n",
    "        obj_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(obj_cols) > 0:\n",
    "            df.rename(columns={obj_cols[0]: 'text'}, inplace=True)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if is_test:\n",
    "        id_candidates = ['id', 'idx', 'index', 'no', 'ID']\n",
    "        id_col = find_column_name(df.columns, id_candidates)\n",
    "        if id_col:\n",
    "            df.rename(columns={id_col: 'id'}, inplace=True)\n",
    "        else:\n",
    "            df['id'] = df.index\n",
    "\n",
    "    if not is_test:\n",
    "        target_candidates = ['generated', 'label', 'target', 'class']\n",
    "        target_col = find_column_name(df.columns, target_candidates)\n",
    "        if target_col:\n",
    "            df.rename(columns={target_col: 'label'}, inplace=True)\n",
    "            try:\n",
    "                df['label'] = df['label'].astype(int)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"타겟(Label) 컬럼을 찾을 수 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    return df\n",
    "\n",
    "def run_process():\n",
    "    set_seeds(SEED)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n[{MODEL_NAME}] 학습 프로세스 시작 (Fixed Dataset Mode)\")\n",
    "\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        print(f\"'{DATA_DIR}' 경로가 없음 / 실행\")\n",
    "        current_data_dir = '/'\n",
    "    else:\n",
    "        current_data_dir = DATA_DIR\n",
    "\n",
    "    train_path = os.path.join(current_data_dir, TRAIN_FILE_NAME)\n",
    "    val_path = os.path.join(current_data_dir, VAL_FILE_NAME)\n",
    "    test_path = os.path.join(current_data_dir, TEST_FILE_NAME)\n",
    "\n",
    "    if not (os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path)):\n",
    "        print(f\"데이터셋 파일을 찾을 수 없습니다.\")\n",
    "        print(f\"   확인 경로:\\n   - {train_path}\\n   - {val_path}\\n   - {test_path}\")\n",
    "        print(\"   파일명 설정 변수를 확인하거나 데이터 분할 코드를 실행하세요.\")\n",
    "        return\n",
    "\n",
    "    print(f\">>> 데이터셋 로드 중...\")\n",
    "    print(f\"   Train: {TRAIN_FILE_NAME}\")\n",
    "    print(f\"   Valid: {VAL_FILE_NAME}\")\n",
    "    print(f\"   Test : {TEST_FILE_NAME}\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    train_df = load_and_fix_data(train_path, is_test=False)\n",
    "    val_df = load_and_fix_data(val_path, is_test=False)\n",
    "    test_df = load_and_fix_data(test_path, is_test=False) \n",
    "\n",
    "    if train_df is None or val_df is None or test_df is None:\n",
    "        print(\"데이터 로드 중 오류 발생. 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\" - Train Set : {len(train_df)}개\")\n",
    "    print(f\" - Valid Set : {len(val_df)}개\")\n",
    "    print(f\" - Test Set  : {len(test_df)}개\")\n",
    "\n",
    "    # Dataset 생성\n",
    "    train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "    val_ds = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "    test_ds = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "    print(f\">>> 토크나이저 로드 ({MODEL_NAME})...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def preprocess(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "    encoded_train = train_ds.map(preprocess, batched=True)\n",
    "    encoded_val = val_ds.map(preprocess, batched=True)\n",
    "    encoded_test = test_ds.map(preprocess, batched=True)\n",
    "\n",
    "    print(f\">>> 모델 로드 ({MODEL_NAME})...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        eval_strategy=\"epoch\",     \n",
    "        save_strategy=\"epoch\",     \n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True, \n",
    "        metric_for_best_model=\"f1\",  \n",
    "        greater_is_better=True,      \n",
    "        save_total_limit=2,       \n",
    "        report_to=\"none\",\n",
    "        seed=SEED\n",
    "    )\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        labels = p.label_ids\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average='macro')\n",
    "        return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=encoded_train, eval_dataset=encoded_val,\n",
    "        tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "    )\n",
    "\n",
    "    print(\">>> 학습 시작...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\">>> Best 모델 저장 중({OUTPUT_DIR})...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    print(\">>> 최종 성능 평가 (Test Set)...\")\n",
    "    metrics = trainer.evaluate(encoded_test)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"Final Test Accuracy : {metrics['eval_accuracy']:.4f}\")\n",
    "    print(f\"Final Test F1 Score : {metrics['eval_f1']:.4f}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "    print(\">>> 상세 예측 리포트 생성 중...\")\n",
    "    preds_output = trainer.predict(encoded_test)\n",
    "    \n",
    "\n",
    "    print(\">>> 앙상블/상관계수 분석용 CSV 생성 중...\")\n",
    "    logits = torch.tensor(preds_output.predictions)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    prob_class_1 = probs[:, 1].numpy()\n",
    "    \n",
    "    correlation_df = test_df.copy()\n",
    "    correlation_df['prob_class_1'] = prob_class_1\n",
    "    correlation_df['pred_label'] = np.argmax(preds_output.predictions, axis=1)\n",
    "    \n",
    "    corr_save_path = os.path.join(OUTPUT_DIR, 'preds_for_correlation.csv')\n",
    "    correlation_df.to_csv(corr_save_path, index=False)\n",
    "    print(f\"저장 완료: {corr_save_path}\")\n",
    "\n",
    "    pred_labels = np.argmax(preds_output.predictions, axis=1)\n",
    "    true_labels = test_df['label'].values\n",
    "    \n",
    "    print(classification_report(true_labels, pred_labels, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "    print(\">>> 오답 분석용 CSV 저장 중...\")\n",
    "    test_df['predicted'] = pred_labels\n",
    "    wrong_df = test_df[test_df['label'] != test_df['predicted']]\n",
    "    wrong_save_path = '/content/wrong_predictions.csv'\n",
    "    wrong_df.to_csv(wrong_save_path, index=False)\n",
    "    print(f\"   오답 데이터 {len(wrong_df)}개 저장 완료: {wrong_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU 연결 성공: {torch.cuda.get_device_name(0)}\")\n",
    "        run_process()\n",
    "    else:\n",
    "        print(\"GPU 감지가 안됨.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a694c",
   "metadata": {},
   "source": [
    "**submission 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "\n",
    "BASE_DIR = '/model'\n",
    "DATA_DIR = '/Data'\n",
    "SAVED_MODEL_NAME = 'klue_roberta_small_result'\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, SAVED_MODEL_NAME)\n",
    "save_csv_path = '/temp_submission.csv'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"모델 로드 경로: {OUTPUT_DIR}\")\n",
    "saved_model_path = OUTPUT_DIR\n",
    "test_data_path = os.path.join(DATA_DIR, 'test.csv')\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    print(f\"'{test_data_path}' 파일이 없음\")\n",
    "    test_data_path = '/test.csv'\n",
    "\n",
    "test_df = load_and_fix_data(test_data_path, is_test=True)\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"테스트 데이터 로드 성공: {len(test_df)}행\")\n",
    "    \n",
    "    try:\n",
    "        loaded_model = AutoModelForSequenceClassification.from_pretrained(saved_model_path)\n",
    "        loaded_tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "        print(\"모델 및 토크나이저 로드 성공\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        print(\"   -> 경로 및 모델 파일이 존재하는지 확인.\")\n",
    "        loaded_model = None\n",
    "\n",
    "    if loaded_model:\n",
    "        test_ds = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "        def token_func(examples):\n",
    "            return loaded_tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN, padding=False)\n",
    "\n",
    "        encoded_test = test_ds.map(token_func, batched=True)\n",
    "\n",
    "        temp_inference_dir = os.path.join(BASE_DIR, 'temp_inference')\n",
    "        \n",
    "        inference_args = TrainingArguments(\n",
    "            output_dir=temp_inference_dir,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            fp16=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        inference_trainer = Trainer(\n",
    "            model=loaded_model,\n",
    "            args=inference_args,\n",
    "            tokenizer=loaded_tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(loaded_tokenizer)\n",
    "        )\n",
    "\n",
    "        print(\">>> 예측 수행 중...\")\n",
    "        pred_output = inference_trainer.predict(encoded_test)\n",
    "\n",
    "        logits = torch.tensor(pred_output.predictions)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        prob_class_1 = probs[:, 1].numpy() \n",
    "\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': test_df['id'],  \n",
    "            'generated': prob_class_1\n",
    "        })\n",
    "\n",
    "        submission.to_csv(save_csv_path, index=False)\n",
    "\n",
    "        print(f\"\\n파일 생성 완료: {save_csv_path}\")\n",
    "        print(submission.head())\n",
    "\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(save_csv_path)\n",
    "        except:\n",
    "            print(\"자동 다운로드 실패\")\n",
    "            pass\n",
    "else:\n",
    "    print(\"test 데이터 로드 실패\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
