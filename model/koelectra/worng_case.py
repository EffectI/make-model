#unset HSA_OVERRIDE_GFX_VERSION
import pandas as pd
import numpy as np
import torch
import re
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

# ==========================================
# [ì„¤ì •] ê²½ë¡œ ë° ëª¨ë¸
# ==========================================
# ì´ì „ ë‹¨ê³„ì—ì„œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
BASE_DIR = Path('/home/user/rocm_project/make-model/model/result_models/koelectra_small')
ERROR_FILE_PATH = BASE_DIR / 'hard_cases_all_wrong.csv'

# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ Foldì˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ë¶„ì„ (ì˜ˆ: fold0)
MODEL_PATH = BASE_DIR / 'fold1'
MODEL_NAME = "monologg/koelectra-small-v3-discriminator"
MAX_LEN = 512

# ==========================================
# [í•¨ìˆ˜] ë¬¸ì¥ ë¶„ë¦¬ ë° ì¶”ë¡ 
# ==========================================
def split_sentences(text):
    #? ! ë’¤ì— ê³µë°±ì´ ì˜¤ë©´ ìë¦„
    # ë¬¸ì¥ ë ë¶€í˜¸ ë’¤ì— ê³µë°±ì´ ìˆëŠ” ê²½ìš° ë¶„ë¦¬
    sentences = re.split(r'(?<=[.?!])\s+', text)
    return [s for s in sentences if len(s.strip()) > 5] # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸

def analyze_sentences():
    # 1. ë°ì´í„° ë° ëª¨ë¸ ë¡œë“œ
    if not ERROR_FILE_PATH.exists():
        print("ì˜¤ë‹µ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        print(f"   ì°¾ëŠ” ê²½ë¡œ: {ERROR_FILE_PATH}")
        return

    df_wrong = pd.read_csv(ERROR_FILE_PATH)
    print(f"ğŸ“‚ ë¶„ì„ ëŒ€ìƒ: {len(df_wrong)}ê°œ (4ê°œ ëª¨ë¸ ëª¨ë‘ í‹€ë¦° ì¼€ì´ìŠ¤)")

    print("â³ ëª¨ë¸ ë¡œë”© ì¤‘...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # ëª¨ë¸ì€ ì €ì¥ëœ ê²½ë¡œì—ì„œ ë¡œë“œ (pytorch_model.binì´ ìˆëŠ” í´ë”)
    model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_PATH))
    model.to(device)
    model.eval()

    results = []

    # 2. ê° ë°ì´í„°ë³„ ë¬¸ì¥ ë¶„ì„
    print("ğŸš€ ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì„ ì‹œì‘ (ë²”ì¸ ìƒ‰ì¶œ ì¤‘...)")
    for idx, row in tqdm(df_wrong.iterrows(), total=len(df_wrong)):
        doc_id = row['id']
        full_text = row['text']
        true_label = row['label']

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = split_sentences(full_text)

        if not sentences:
            continue

        # ë¬¸ì¥ë³„ ì¶”ë¡ 
        inputs = tokenizer(sentences, truncation=True, padding=True, max_length=128, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            # outputs.logitsë¡œ ì ‘ê·¼í•˜ëŠ” ëŒ€ì‹  íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            logits = outputs[0]
            probs = torch.nn.functional.softmax(logits, dim=-1)
            probs_1 = probs[:, 1].cpu().numpy() # AIì¼ í™•ë¥ 

        # ê°€ì¥ AIìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ì°¾ê¸°
        max_prob_idx = np.argmax(probs_1)
        max_prob = probs_1[max_prob_idx]
        culprit_sentence = sentences[max_prob_idx]

        results.append({
            'id': doc_id,
            'max_prob_ai': max_prob, # ì´ ë¬¸ì¥ì´ AIì¼ í™•ë¥ 
            'culprit_sentence': culprit_sentence, # ë¬¸ì œì˜ ê·¸ ë¬¸ì¥
            'full_text_preview': full_text[:50] + "..."
        })

    # 3. ê²°ê³¼ ì €ì¥
    result_df = pd.DataFrame(results)
    # AI í™•ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    result_df = result_df.sort_values(by='max_prob_ai', ascending=False)

    save_path = BASE_DIR / 'culprit_sentences_analysis.csv'
    result_df.to_csv(save_path, index=False, encoding='utf-8-sig')

    print("\n" + "="*50)
    print(f"ë¶„ì„ ì™„ë£Œ ì €ì¥ ê²½ë¡œ: {save_path}")
    print("="*50)

    # ìƒìœ„ 3ê°œ ë¯¸ë¦¬ë³´ê¸°
    print("\n[TOP 3 sentences likely to be AI-generated]")
    for i in range(min(3, len(result_df))):
        row = result_df.iloc[i]
        print(f"\n{i+1}ìœ„ (AI í™•ë¥ : {row['max_prob_ai']:.4f})")
        print(f"   ë¬¸ì¥: \"{row['culprit_sentence']}\"")
        print(f"   ì›ë³¸ID: {row['id']}")

if __name__ == "__main__":
    analyze_sentences()