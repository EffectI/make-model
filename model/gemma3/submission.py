import os
import gc
import torch
import pandas as pd
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ==========================================
# [1] ì„¤ì •
# ==========================================
TEST_FILE_PATH = '/home/user/rocm_project/make-model/data/raw/test.csv'
MODEL_ROOT_DIR = '/home/user/rocm_project/make-model/model/result_models/koelectra_small'
OUTPUT_SUBMISSION_PATH = './submission_ensemble.csv'

N_FOLDS = 4
BATCH_SIZE = 64
MAX_LEN = 512
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"âš™ï¸ ì„¤ì • í™•ì¸:")
print(f"   - Input: {TEST_FILE_PATH}")
print(f"   - Model Root: {MODEL_ROOT_DIR}")
print(f"   - Device: {DEVICE}")

# ==========================================
# [2] ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==========================================
class TestDataset(Dataset):
    def __init__(self, df, tokenizer, max_len):
        self.texts = df['text'].values
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, index):
        text = str(self.texts[index])
        inputs = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0)
        }

# ==========================================
# [3] ì¶”ë¡  í•¨ìˆ˜ (ìˆ˜ì •ë¨: Tuple/Logits í˜¸í™˜)
# ==========================================
def inference(model_path, test_loader):
    print(f"   Derived from: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.to(DEVICE)
    model.eval()
    
    predictions = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="   Predicting"):
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            
            # -----------------------------------------------------------
            # [Fix] ì¶œë ¥ì´ Tupleì¸ ê²½ìš°ì™€ Objectì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            # -----------------------------------------------------------
            if hasattr(outputs, 'logits'):
                logits = outputs.logits
            else:
                logits = outputs[0]  # Tupleì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œê°€ logits
            
            # Softmax: Logits -> Probability
            probs = torch.nn.functional.softmax(logits, dim=-1)
            predictions.append(probs.cpu().numpy())
            
    del model, tokenizer
    torch.cuda.empty_cache()
    gc.collect()
    
    return np.concatenate(predictions, axis=0)

# ==========================================
# [4] ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    # 1. ë°ì´í„° ë¡œë“œ
    if not os.path.exists(TEST_FILE_PATH):
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {TEST_FILE_PATH}")
        return

    test_df = pd.read_csv(TEST_FILE_PATH)
    print(f"ğŸ“‚ Test Data Loaded: {len(test_df)} rows")

    # (1) Text ì»¬ëŸ¼ ì°¾ê¸°
    text_col = None
    candidates = ['text', 'paragraph_text', 'content', 'sentence', 'full_text', 'overview']
    for col in test_df.columns:
        if col.lower() in candidates:
            text_col = col
            break
    
    if text_col:
        test_df.rename(columns={text_col: 'text'}, inplace=True)
    else:
        obj_cols = test_df.select_dtypes(include=['object']).columns
        if len(obj_cols) > 0:
            test_df.rename(columns={obj_cols[0]: 'text'}, inplace=True)
        else:
            print("âŒ [Critical] í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

    # (2) ID ì»¬ëŸ¼ ì°¾ê¸°
    input_id_col = None
    id_candidates = ['id', 'ID', 'idx', 'index', 'no']
    for col in test_df.columns:
        if col in id_candidates: 
            input_id_col = col
            break
    
    # 2. í† í¬ë‚˜ì´ì € ì¤€ë¹„
    first_fold_path = os.path.join(MODEL_ROOT_DIR, "fold0")
    try:
        base_tokenizer = AutoTokenizer.from_pretrained(first_fold_path)
    except:
        print("âš ï¸ ë¡œì»¬ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨. HuggingFace Hub(KoElectra)ì—ì„œ ë¡œë“œ ì‹œë„.")
        base_tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")

    test_dataset = TestDataset(test_df, base_tokenizer, MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

    # 3. 4-Fold ì•™ìƒë¸”
    final_probs = np.zeros((len(test_df), 2))
    success_folds = 0

    for fold in range(N_FOLDS):
        fold_model_dir = os.path.join(MODEL_ROOT_DIR, f"fold{fold}")
        
        if not os.path.exists(fold_model_dir):
             print(f"âš ï¸ [Skip] ëª¨ë¸ í´ë” ì—†ìŒ: {fold_model_dir}")
             continue
             
        print(f"\nğŸ”„ [Fold {fold}] Inference Start...")
        try:
            fold_probs = inference(fold_model_dir, test_loader)
            final_probs += fold_probs
            success_folds += 1
        except Exception as e:
            print(f"âŒ [Error] Fold {fold} ì¶”ë¡  ì‹¤íŒ¨: {e}")

    if success_folds == 0:
        print("âŒ ëª¨ë“  Fold ì¶”ë¡  ì‹¤íŒ¨.")
        return

    # 4. ê²°ê³¼ ì €ì¥
    avg_probs = final_probs / success_folds
    final_preds = np.argmax(avg_probs, axis=1)

    submission = pd.DataFrame()
    
    if input_id_col:
        submission['ID'] = test_df[input_id_col]
    else:
        submission['ID'] = test_df.index

    submission['generated'] = final_preds

    submission.to_csv(OUTPUT_SUBMISSION_PATH, index=False)
    print(f"\nâœ… Submission Saved: {OUTPUT_SUBMISSION_PATH}")
    print(submission.head())

if __name__ == "__main__":
    main()